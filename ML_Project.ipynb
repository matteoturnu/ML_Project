{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import random\n",
        "from importlib.util import find_spec\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn # Contains Required functions and layers\n",
        "import torch.nn.functional as F # For neural network functions:\n",
        "import torch.optim as optim # Contains Optimization function available in PyTorch.\n",
        "project_folder = \"/content/password_strength_classifier\""
      ],
      "metadata": {
        "id": "KykGU56SuABu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of useful directories and download of password dictionaries"
      ],
      "metadata": {
        "id": "V8T3qe2zMghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, dest_folder_name):\n",
        "  local_filename = url.split('/')[-1]\n",
        "  path = os.path.join(\"/{}/{}\".format(dest_folder_name, local_filename))\n",
        "  \"\"\"with requests.get(url, stream=True) as r:\n",
        "      with open(path, 'wb') as f:\n",
        "          shutil.copyfileobj(r.raw, f)\"\"\"\n",
        "\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(requests.get(url, stream=True).content)\n",
        "\n",
        "  # return local_filename\n",
        "  return path\n",
        "\n",
        "def read_file(filepath):\n",
        "  with open(filepath, errors='replace', encoding='utf-8') as f:\n",
        "    data = {line.split('\\n')[0] for line in f.readlines()}\n",
        "  return data\n",
        "\n",
        "\n",
        "if os.path.exists(project_folder) is False:\n",
        "  dict_dir = project_folder + \"/dictionaries/\"\n",
        "  dataset_dir = project_folder + \"/dataset/\"\n",
        "\n",
        "  os.mkdir(project_folder)\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "  os.mkdir(dict_dir)\n",
        "\n",
        "  f_rockyou = download_file(\n",
        "    \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\",\n",
        "    dict_dir)\n",
        "\n",
        "  f_jtr = download_file(\n",
        "      \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Software/john-the-ripper.txt\",\n",
        "      dict_dir)\n",
        "\n",
        "\n",
        "data_rockyou = read_file(f_rockyou)\n",
        "data_jtr = read_file(f_jtr)\n",
        "\n",
        "print(\"\\nRock You\")\n",
        "import itertools\n",
        "print([val for i, val in enumerate(itertools.islice(data_rockyou, 5))])\n",
        "print(\"\\nJohn The Ripper\")\n",
        "print([val for i, val in enumerate(itertools.islice(data_jtr, 5))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynOVFhD_5q3r",
        "outputId": "15a6bf37-ffbb-46e9-e64e-049f347a871f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rock You\n",
            "['', '07881951589', 'fufyfufy', 'eligla', 'mizsammie']\n",
            "\n",
            "John The Ripper\n",
            "['popeye', 'overkill', 'Abcdefg', 'timothy', 'gregory']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "\n",
        "if find_spec(\"kaggle\") is None:\n",
        "  ! pip install -q kaggle\n",
        "\n",
        "if os.path.isdir(\"/root/.kaggle\") is False:\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! touch \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "  token = {\"username\":\"matteoturnu\",\"key\":\"79ea644685a3e574038b40e4019b0927\"}\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d bhavikbb/password-strength-classifier-dataset -p $dataset_dir"
      ],
      "metadata": {
        "id": "y0pIiUJswG7d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"password-strength-classifier-dataset.zip\")\n",
        "\n",
        "pswd_df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "print(pswd_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jiwHeBuGRgP",
        "outputId": "90ae8661-eab4-4b5f-a780-0e25d5e4c0db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669640 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "_VFws5SNT4xt",
        "outputId": "b8896bcc-7eb6-411d-9e0c-3e149dd5a326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669639 rows x 2 columns]\n",
            "[['kzde5577' 1]\n",
            " ['kino3434' 1]\n",
            " ['visi7k1yr' 1]\n",
            " ...\n",
            " ['184520socram' 1]\n",
            " ['marken22a' 1]\n",
            " ['fxx4pw4g' 1]]\n",
            "Len of passwords:  669639\n",
            "Len of UNIQUE passwords:  669639\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vectorizer = TfidfVectorizer(tokenizer=word_split)\\nX = vectorizer.fit_transform(passwords)\\n\\nprint(vectorizer.get_feature_names_out())'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def word_split(inputs):\n",
        "    character=[]\n",
        "    for i in inputs:\n",
        "        character.append(i)\n",
        "    return character\n",
        "\n",
        "# unique values of strength feature\n",
        "pswd_df['strength'].unique()\n",
        "\n",
        "# number of missing values in dataset\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "# remove missing values\n",
        "pswd_df.dropna(inplace=True)\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "print(pswd_df)\n",
        "\n",
        "psw_array = np.array(pswd_df)\n",
        "print(psw_array)\n",
        "\n",
        "# ??? PROBLEM: it is shown that if shuffled then\n",
        "# there are several duplicates of a password\n",
        "#random.shuffle(psw_array)\n",
        "\n",
        "labels = np.array([p[1] for p in psw_array])\n",
        "passwords = np.array([p[0] for p in psw_array])\n",
        "\n",
        "print(\"Len of passwords: \", len(passwords))\n",
        "print(\"Len of UNIQUE passwords: \", len(np.unique(passwords)))\n",
        "\n",
        "\n",
        "\"\"\"vectorizer = TfidfVectorizer(tokenizer=word_split)\n",
        "X = vectorizer.fit_transform(passwords)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Calculate characters weights"
      ],
      "metadata": {
        "id": "0XE_dLdSqh5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary containing occurrencies of all the characters in the dataset\n",
        "def calculate_occurrencies(passwords):\n",
        "  tot_occurrencies = 0\n",
        "  occurrencies_dict = dict()\n",
        "  for password in passwords:\n",
        "    for character in password:\n",
        "      tot_occurrencies += 1\n",
        "      if character in occurrencies_dict:\n",
        "        occurrencies_dict[character] += 1\n",
        "      else:\n",
        "        occurrencies_dict[character] = 1\n",
        "  return occurrencies_dict, tot_occurrencies\n",
        "\n",
        "# Converts occurrencies to weights\n",
        "def calculate_weights(occurrencies_dict, tot_occurrencies):\n",
        "  for key in occurrencies_dict:\n",
        "    weights_dict = dict()\n",
        "    weights_dict[key] = 1 - (occurrencies_dict[key] / tot_occurrencies)\n",
        "  return occurrencies_dict\n",
        "\n",
        "occurrencies_dict, tot_occurrencies = calculate_occurrencies(passwords)\n",
        "weights_dict = calculate_weights(occurrencies_dict, tot_occurrencies)\n",
        "print(weights_dict)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9qsx0MWqnJa",
        "outputId": "9d7e8766-87df-4490-de1d-4d772557c94b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'k': 136460, 'z': 90439, 'd': 130297, 'e': 286511, '5': 169914, '7': 150426, 'i': 253594, 'n': 204531, 'o': 249369, '3': 214671, '4': 169901, 'v': 81262, 's': 193446, '1': 385609, 'y': 130769, 'r': 199364, 'm': 160353, 'g': 126334, '2': 280804, 'l': 168557, 'a': 408272, 'b': 104639, 'h': 117154, 'A': 43221, 'V': 12445, 'Y': 21203, 'q': 55139, 'D': 29536, 'E': 24424, 'M': 40542, 'Z': 12565, 'f': 77724, 'N': 35061, 't': 159369, 'u': 160714, '6': 150335, 'c': 129623, '8': 162215, 'w': 97269, '9': 196584, '0': 250268, 'j': 95251, 'W': 12621, 'U': 21104, 'I': 23029, 'O': 28219, 'Q': 32503, 'P': 13419, 'p': 111185, '@': 5448, '-': 3146, 'H': 13325, 'x': 69488, '.': 6178, 'T': 29780, '>': 94, 'G': 13779, 'J': 13414, '&': 664, '?': 501, '<': 118, '!': 2182, 'S': 15705, 'R': 15063, 'F': 12952, 'B': 13443, 'K': 13569, 'X': 12424, 'C': 13823, 'L': 14507, ';': 363, '_': 2927, '%': 409, '±': 133, '\"': 9, '~': 93, '+': 692, '^': 338, '/': 834, '$': 1140, ')': 365, ' ': 1110, '(': 315, '#': 1208, 'Ú': 26, '*': 1877, '`': 13, '{': 39, '}': 37, '[': 150, ']': 132, 'þ': 24, 'Þ': 8, 'Ó': 17, 'Ô': 5, '=': 240, '\\\\': 40, '\\x1c': 5, '³': 24, '¿': 5, '\\x16': 3, 'Ò': 2, '·': 4, '\\x1e': 2, '\\x19': 1, '\\x05': 3, '\\x1b': 2, 'Å': 13, '‚': 1, 'Ä': 49, 'à': 2, 'õ': 13, 'ß': 10, '´': 2, '«': 1, 'ð': 5, 'Ð': 20, 'å': 6, 'â': 2, '°': 12, '|': 37, '\\x7f': 2, '²': 2, '¾': 6, 'Ÿ': 13, '\\x08': 1, 'ê': 1, 'á': 5, '\\x10': 4, '\\x17': 4, 'º': 10, '¡': 3, '÷': 17, 'Õ': 2, 'í': 2, 'ú': 6, 'µ': 4, 'Ý': 4, 'Ü': 3, 'Û': 2, 'Ö': 2, '×': 1, '¨': 4, '\\xa0': 2, 'æ': 3, 'è': 1, 'ù': 2, 'É': 1, '\\x06': 2, 'Ñ': 4, '\\x81': 2, '\\x11': 2, 'Â': 3, 'ý': 1, '—': 2, '›': 2, '‹': 1, 'œ': 1, '™': 1, 'ó': 1, '¦': 1, '\\x0f': 1, 'ä': 1, 'ï': 1, 'Ï': 1, 'Ç': 7, 'ò': 1, 'ö': 1, '\\x12': 4, '\\x8d': 2, 'î': 1, '¹': 1, '¶': 1, 'ñ': 1, '¼': 2, 'Ù': 1, '…': 1, '\\x13': 1, '\\x1d': 1, '\\x04': 1, '\\x0e': 1, '\\x02': 1, '¯': 1, '\\x01': 1, 'Ê': 1, 'Æ': 2, '‡': 1, '¤': 1, 'À': 1, '¢': 1, 'û': 1, 'ƒ': 1, 'é': 1, 'Á': 1, '§': 1, 'Í': 2, 'Ã': 1, '»': 1, '\\x18': 1, '½': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LENGTH feature\n",
        "# lengths = np.array([len(p) for p in passwords]).reshape(-1, 1)\n",
        "lengths = np.array([len(p) for p in passwords])\n",
        "max_len = np.max(lengths)\n",
        "min_len = np.min(lengths)\n",
        "\n",
        "print(\"Lengths shape: \", lengths.shape)\n",
        "\n",
        "print(lengths)\n",
        "print(f\"Max: {max_len} --> {passwords[lengths == max_len]} \\nMin: {min_len} --> {passwords[lengths == min_len]}\")\n",
        "\n",
        "# normalize() from sklearn accept 2D arrays only --> reshape so that we have 1 row (1 \"sample\")\n",
        "# and make sklearn compute the remaining number of columns for us\n",
        "feat_length = normalize(lengths.reshape(1, -1))\n",
        "\n",
        "#feat_length = feat_length.flatten()\n",
        "print(feat_length)\n",
        "\n",
        "\n",
        "# ROCKYOU feature\n",
        "\n",
        "# numpy array of 1 and 0 (0 if found, otherwise is 1)\n",
        "# int() used to convert boolean into number\n",
        "feat_rockyou = np.array([int(p not in data_rockyou) for p in passwords])\n",
        "print(feat_rockyou)\n",
        "\n",
        "# passwords found in rockyou file\n",
        "print(passwords[feat_rockyou == 0])\n",
        "\n",
        "\n",
        "# JTR feature\n",
        "feat_jtr = np.array([int(p not in data_jtr) for p in passwords])\n",
        "print(feat_jtr)\n",
        "print(passwords[feat_jtr == 0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awONOEU9HOhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password structure feature"
      ],
      "metadata": {
        "id": "uxIR4HNJgG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define macros\n",
        "numbers = 0\n",
        "lower_case = 1\n",
        "upper_case = 2\n",
        "special_char = 3\n",
        "\n",
        "# Counts numbers, lowercases, uppercases and other characters in a password\n",
        "def calculate_password_structure(passwords):\n",
        "  passwords_structure = []\n",
        "  for password in passwords:\n",
        "    counts = np.array([0, 0, 0, 0])\n",
        "    for character in password:\n",
        "      if character.isnumeric():\n",
        "        counts[numbers] += 1\n",
        "      elif character.islower():\n",
        "        counts[lower_case] += 1\n",
        "      elif character.isupper():\n",
        "        counts[upper_case] += 1\n",
        "      else:\n",
        "        counts[special_char] += 1\n",
        "\n",
        "    passwords_structure.append(counts / len(password))\n",
        "  return np.array(passwords_structure)\n",
        "\n",
        "feat_structure = calculate_password_structure(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_structure[i])"
      ],
      "metadata": {
        "id": "wLptS-t0gAEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password score feature by characters appearance frequency"
      ],
      "metadata": {
        "id": "Klkyyk59omsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate passwords score with character weights\n",
        "def calculate_password_scores(passwords):\n",
        "  scores = []\n",
        "  for password in passwords:\n",
        "    score = 0\n",
        "    for character in password:\n",
        "      # If weight does not exists in the dictionary, it is a very rare character so 1.001 points are given\n",
        "      try:\n",
        "        score += weights_dict[character]\n",
        "      except:\n",
        "        score += 1\n",
        "    scores.append(score)\n",
        "\n",
        "  return np.array(scores)\n",
        "\n",
        "feat_scores = calculate_password_scores(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_scores[i])"
      ],
      "metadata": {
        "id": "T3vr_4C-o06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Create the feature vector\n",
        "# Currently we are not including \"feat_score\"\n",
        "\n",
        "# divide \"feat_structure\" in its sub-features since hstack() needs the same\n",
        "# dimension for every array\n",
        "\n",
        "def create_feature_vector(features_list):\n",
        "  # 4 features in the list\n",
        "\n",
        "\n",
        "  for feature in features_list:\n",
        "    if len(feature.shape) != 1: # it's 2-d feature_structure or feat_length\n",
        "      if abs(feature.shape[1]) > 1: # it's the 2-d feature_structure --> decompose it to 4 different sub-features\n",
        "\n",
        "\n",
        "\n",
        "  features_vector = np.hstack(feature)\n",
        "  return feature_vector\n",
        "\n",
        "print(\"\\nFeature length: \", feat_length)\n",
        "print(\"\\nFeature rockyou: \", feat_rockyou)\n",
        "print(\"\\nFeature jtr: \", feat_jtr)\n",
        "print(\"\\nFeature structure --> numbers: \", feat_structure[:, 0])\n",
        "print(\"\\nFeature structure --> lowercase: \", feat_structure[:, 1])\n",
        "print(\"\\nFeature structure --> uppercase: \", feat_structure[:, 2])\n",
        "print(\"\\nFeature structure --> special: \", feat_structure[:, 3])\n",
        "\n",
        "print(\"feature shape: \", len(feat_length.shape))\n",
        "\n",
        "\n",
        "feature_vector = np.hstack((feat_length, feat_rockyou, feat_jtr, feat_structure[:, 0],\n",
        "                            feat_structure[:, 1], feat_structure[:, 2], feat_structure[:, 3]))\n",
        "print(feature_vector)\n",
        "print(feature_vector.shape)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OzjdbbivBUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Neural Network"
      ],
      "metadata": {
        "id": "P2Ovy96LylJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Dataset Creation"
      ],
      "metadata": {
        "id": "I7XC8I8m-CAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "rEQlGb3Y-FES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Trainloader creation"
      ],
      "metadata": {
        "id": "WqLB76e--R4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_DataLoader(trainset, testset):\n",
        "  # trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "  # No need to shuffle test data.\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "1_bBkaz_-V0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.mid_n = int((n_input + n_output) / 2)\n",
        "\n",
        "    # Define Layers:\n",
        "    self.l1 = nn.Linear(self.n_input, self.mid_n) # layer 1\n",
        "    self.l2 = nn.Linear(self.mid_n, self.n_output) # layer 2\n",
        "    self.l3 = nn.Linear(self.n_output, self.n_output) # layer 3\n",
        "\n",
        "    # Define Activation functions:\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # Weights initialization\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='relu') # Using HE because more optimized for ReLU activated layers\n",
        "    nn.init.zeros_(self.l1.bias)\n",
        "    nn.init.kaiming_normal_(self.l2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.l2.bias)\n",
        "    nn.init.normal_(self.l3.weight) # Using normal distribution for LogSoftMax activated layer\n",
        "    nn.init.normal_(self.l3.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Layers: 3\n",
        "    Activation Functions:\n",
        "    RELU for first two layers\n",
        "    Log Softmax for last layer\n",
        "    '''\n",
        "    x = self.l1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def train_model(self, tr_loader, n_epochs, criterion, optimizer):\n",
        "    # losses --> {[*idx_first_epoch*]: loss_1, [*idx_second_epoch*]: loss_2, ...}\n",
        "    losses = {}\n",
        "    for e in range(n_epochs):\n",
        "      for features, labels in tr_loader:\n",
        "        optimizer.zero_grad() # set optimizer gradients to zero:\n",
        "        output = self(features) # Intial output\n",
        "        loss = criterion(output, labels.long()) # Loss Caluclation\n",
        "        loss.backward() # Pass loss function gradients to pervious layers:\n",
        "        optimizer.step() # Update Weights\n",
        "        losses[e] = loss.item()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "  def test_model(self, ts_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in ts_loader:\n",
        "      outputs = self(features)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "# Define macros.\n",
        "number_of_features = 8 # Rockyou, johntheripper , password len, numbers/len, lowercases/len, uppercases/len, special characters/len, password score\n",
        "number_of_outputs = 3 # Unsecure, intermediate, secure\n",
        "\n",
        "# Initialize NN\n",
        "NN = NeuralNetwork(number_of_features, number_of_outputs)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(NN.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4FDFyMMQypbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}