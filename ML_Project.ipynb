{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import random\n",
        "from importlib.util import find_spec\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn # Contains Required functions and layers\n",
        "import torch.nn.functional as F # For neural network functions:\n",
        "import torch.optim as optim # Contains Optimization function available in PyTorch.\n",
        "project_folder = \"/content/password_strength_classifier\""
      ],
      "metadata": {
        "id": "KykGU56SuABu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of useful directories and download of password dictionaries"
      ],
      "metadata": {
        "id": "V8T3qe2zMghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, dest_folder_name):\n",
        "  local_filename = url.split('/')[-1]\n",
        "  path = os.path.join(\"/{}/{}\".format(dest_folder_name, local_filename))\n",
        "  \"\"\"with requests.get(url, stream=True) as r:\n",
        "      with open(path, 'wb') as f:\n",
        "          shutil.copyfileobj(r.raw, f)\"\"\"\n",
        "\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(requests.get(url, stream=True).content)\n",
        "\n",
        "  # return local_filename\n",
        "  return path\n",
        "\n",
        "def read_file(filepath):\n",
        "  with open(filepath, errors='replace', encoding='utf-8') as f:\n",
        "    data = {line.split('\\n')[0] for line in f.readlines()}\n",
        "  return data\n",
        "\n",
        "\n",
        "if os.path.exists(project_folder) is False:\n",
        "  dict_dir = project_folder + \"/dictionaries/\"\n",
        "  dataset_dir = project_folder + \"/dataset/\"\n",
        "\n",
        "  os.mkdir(project_folder)\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "  os.mkdir(dict_dir)\n",
        "\n",
        "  f_rockyou = download_file(\n",
        "    \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\",\n",
        "    dict_dir)\n",
        "\n",
        "  f_jtr = download_file(\n",
        "      \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Software/john-the-ripper.txt\",\n",
        "      dict_dir)\n",
        "\n",
        "\n",
        "data_rockyou = read_file(f_rockyou)\n",
        "data_jtr = read_file(f_jtr)\n",
        "\n",
        "print(\"\\nRock You\")\n",
        "import itertools\n",
        "print([val for i, val in enumerate(itertools.islice(data_rockyou, 5))])\n",
        "print(\"\\nJohn The Ripper\")\n",
        "print([val for i, val in enumerate(itertools.islice(data_jtr, 5))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynOVFhD_5q3r",
        "outputId": "8459b941-28e5-45e7-8c3d-efc462fadb30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rock You\n",
            "['', 'myshell30', 'steph26!', 'revival08', 'name1993']\n",
            "\n",
            "John The Ripper\n",
            "['alison', 'topcat', 'hornet', 'willy', 'clancy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "\n",
        "if find_spec(\"kaggle\") is None:\n",
        "  ! pip install -q kaggle\n",
        "\n",
        "if os.path.isdir(\"/root/.kaggle\") is False:\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! touch \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "  token = {\"username\":\"matteoturnu\",\"key\":\"79ea644685a3e574038b40e4019b0927\"}\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d bhavikbb/password-strength-classifier-dataset -p $dataset_dir"
      ],
      "metadata": {
        "id": "y0pIiUJswG7d",
        "outputId": "9357ea29-ced1-4e3e-e95e-eaf67d312981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading password-strength-classifier-dataset.zip to /content/password_strength_classifier/dataset\n",
            "\r  0% 0.00/5.01M [00:00<?, ?B/s]\n",
            "\r100% 5.01M/5.01M [00:00<00:00, 74.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read file\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"password-strength-classifier-dataset.zip\")\n",
        "\n",
        "pswd_df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "print(pswd_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jiwHeBuGRgP",
        "outputId": "aab870c4-23bf-4131-919c-06776f497dc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669640 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "_VFws5SNT4xt",
        "outputId": "34003fd4-c1b0-4756-d825-1861cdd10768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669639 rows x 2 columns]\n",
            "[['kzde5577' 1]\n",
            " ['kino3434' 1]\n",
            " ['visi7k1yr' 1]\n",
            " ...\n",
            " ['184520socram' 1]\n",
            " ['marken22a' 1]\n",
            " ['fxx4pw4g' 1]]\n",
            "Len of passwords:  669639\n",
            "Len of UNIQUE passwords:  669639\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vectorizer = TfidfVectorizer(tokenizer=word_split)\\nX = vectorizer.fit_transform(passwords)\\n\\nprint(vectorizer.get_feature_names_out())'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def word_split(inputs):\n",
        "    character=[]\n",
        "    for i in inputs:\n",
        "        character.append(i)\n",
        "    return character\n",
        "\n",
        "# unique values of strength feature\n",
        "pswd_df['strength'].unique()\n",
        "\n",
        "# number of missing values in dataset\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "# remove missing values\n",
        "pswd_df.dropna(inplace=True)\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "print(pswd_df)\n",
        "\n",
        "psw_array = np.array(pswd_df)\n",
        "print(psw_array)\n",
        "\n",
        "# ??? PROBLEM: it is shown that if shuffled then\n",
        "# there are several duplicates of a password\n",
        "#random.shuffle(psw_array)\n",
        "\n",
        "labels = np.array([p[1] for p in psw_array])\n",
        "passwords = np.array([p[0] for p in psw_array])\n",
        "\n",
        "print(\"Len of passwords: \", len(passwords))\n",
        "print(\"Len of UNIQUE passwords: \", len(np.unique(passwords)))\n",
        "\n",
        "n_samples = len(passwords)\n",
        "n_features = 8\n",
        "\n",
        "\"\"\"vectorizer = TfidfVectorizer(tokenizer=word_split)\n",
        "X = vectorizer.fit_transform(passwords)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Calculate characters weights"
      ],
      "metadata": {
        "id": "0XE_dLdSqh5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary containing occurrencies of all the characters in the dataset\n",
        "def calculate_occurrencies(passwords):\n",
        "  tot_occurrencies = 0\n",
        "  occurrencies_dict = dict()\n",
        "  for password in passwords:\n",
        "    for character in password:\n",
        "      tot_occurrencies += 1\n",
        "      if character in occurrencies_dict:\n",
        "        occurrencies_dict[character] += 1\n",
        "      else:\n",
        "        occurrencies_dict[character] = 1\n",
        "  return occurrencies_dict, tot_occurrencies\n",
        "\n",
        "# Converts occurrencies to weights\n",
        "def calculate_weights(occurrencies_dict, tot_occurrencies):\n",
        "  for key in occurrencies_dict:\n",
        "    weights_dict = dict()\n",
        "    weights_dict[key] = 1 - (occurrencies_dict[key] / tot_occurrencies)\n",
        "  return occurrencies_dict\n",
        "\n",
        "occurrencies_dict, tot_occurrencies = calculate_occurrencies(passwords)\n",
        "weights_dict = calculate_weights(occurrencies_dict, tot_occurrencies)\n",
        "print(weights_dict)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9qsx0MWqnJa",
        "outputId": "a88a1b5e-cf9d-4394-9a50-1122245294f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'k': 136460, 'z': 90439, 'd': 130297, 'e': 286511, '5': 169914, '7': 150426, 'i': 253594, 'n': 204531, 'o': 249369, '3': 214671, '4': 169901, 'v': 81262, 's': 193446, '1': 385609, 'y': 130769, 'r': 199364, 'm': 160353, 'g': 126334, '2': 280804, 'l': 168557, 'a': 408272, 'b': 104639, 'h': 117154, 'A': 43221, 'V': 12445, 'Y': 21203, 'q': 55139, 'D': 29536, 'E': 24424, 'M': 40542, 'Z': 12565, 'f': 77724, 'N': 35061, 't': 159369, 'u': 160714, '6': 150335, 'c': 129623, '8': 162215, 'w': 97269, '9': 196584, '0': 250268, 'j': 95251, 'W': 12621, 'U': 21104, 'I': 23029, 'O': 28219, 'Q': 32503, 'P': 13419, 'p': 111185, '@': 5448, '-': 3146, 'H': 13325, 'x': 69488, '.': 6178, 'T': 29780, '>': 94, 'G': 13779, 'J': 13414, '&': 664, '?': 501, '<': 118, '!': 2182, 'S': 15705, 'R': 15063, 'F': 12952, 'B': 13443, 'K': 13569, 'X': 12424, 'C': 13823, 'L': 14507, ';': 363, '_': 2927, '%': 409, '±': 133, '\"': 9, '~': 93, '+': 692, '^': 338, '/': 834, '$': 1140, ')': 365, ' ': 1110, '(': 315, '#': 1208, 'Ú': 26, '*': 1877, '`': 13, '{': 39, '}': 37, '[': 150, ']': 132, 'þ': 24, 'Þ': 8, 'Ó': 17, 'Ô': 5, '=': 240, '\\\\': 40, '\\x1c': 5, '³': 24, '¿': 5, '\\x16': 3, 'Ò': 2, '·': 4, '\\x1e': 2, '\\x19': 1, '\\x05': 3, '\\x1b': 2, 'Å': 13, '‚': 1, 'Ä': 49, 'à': 2, 'õ': 13, 'ß': 10, '´': 2, '«': 1, 'ð': 5, 'Ð': 20, 'å': 6, 'â': 2, '°': 12, '|': 37, '\\x7f': 2, '²': 2, '¾': 6, 'Ÿ': 13, '\\x08': 1, 'ê': 1, 'á': 5, '\\x10': 4, '\\x17': 4, 'º': 10, '¡': 3, '÷': 17, 'Õ': 2, 'í': 2, 'ú': 6, 'µ': 4, 'Ý': 4, 'Ü': 3, 'Û': 2, 'Ö': 2, '×': 1, '¨': 4, '\\xa0': 2, 'æ': 3, 'è': 1, 'ù': 2, 'É': 1, '\\x06': 2, 'Ñ': 4, '\\x81': 2, '\\x11': 2, 'Â': 3, 'ý': 1, '—': 2, '›': 2, '‹': 1, 'œ': 1, '™': 1, 'ó': 1, '¦': 1, '\\x0f': 1, 'ä': 1, 'ï': 1, 'Ï': 1, 'Ç': 7, 'ò': 1, 'ö': 1, '\\x12': 4, '\\x8d': 2, 'î': 1, '¹': 1, '¶': 1, 'ñ': 1, '¼': 2, 'Ù': 1, '…': 1, '\\x13': 1, '\\x1d': 1, '\\x04': 1, '\\x0e': 1, '\\x02': 1, '¯': 1, '\\x01': 1, 'Ê': 1, 'Æ': 2, '‡': 1, '¤': 1, 'À': 1, '¢': 1, 'û': 1, 'ƒ': 1, 'é': 1, 'Á': 1, '§': 1, 'Í': 2, 'Ã': 1, '»': 1, '\\x18': 1, '½': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LENGTH feature\n",
        "# lengths = np.array([len(p) for p in passwords]).reshape(-1, 1)\n",
        "lengths = np.array([len(p) for p in passwords])\n",
        "max_len = np.max(lengths)\n",
        "min_len = np.min(lengths)\n",
        "\n",
        "print(\"Lengths shape: \", lengths.shape)\n",
        "\n",
        "print(lengths)\n",
        "print(f\"Max: {max_len} --> {passwords[lengths == max_len]} \\nMin: {min_len} --> {passwords[lengths == min_len]}\")\n",
        "\n",
        "# normalize() from sklearn accept 2D arrays only --> reshape so that we have 1 row (1 \"sample\")\n",
        "# and make sklearn compute the remaining number of columns for us\n",
        "feat_length = normalize(lengths.reshape(1, -1))\n",
        "\n",
        "#feat_length = feat_length.flatten()\n",
        "print(feat_length)\n",
        "\n",
        "\n",
        "# ROCKYOU feature\n",
        "\n",
        "# numpy array of 1 and 0 (0 if found, otherwise is 1)\n",
        "# int() used to convert boolean into number\n",
        "feat_rockyou = np.array([int(p not in data_rockyou) for p in passwords])\n",
        "print(feat_rockyou)\n",
        "\n",
        "# passwords found in rockyou file\n",
        "print(passwords[feat_rockyou == 0])\n",
        "\n",
        "\n",
        "# JTR feature\n",
        "feat_jtr = np.array([int(p not in data_jtr) for p in passwords])\n",
        "print(feat_jtr)\n",
        "print(passwords[feat_jtr == 0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awONOEU9HOhF",
        "outputId": "d3f82967-75f3-47b4-b0d7-d107bc28d279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths shape:  (669639,)\n",
            "[ 8  8  9 ... 12  9  8]\n",
            "Max: 220 --> ['In0LnUoff8wfayJGqzelyDqg4AMl9gBhgl3T2iZeONzh5gPqTyP8IVLsQ960aZwlZcdSjE1XCi8taVT5dWSB3wNJwMqpzmlSIKh21A8TNxpSJ5nu2hULRgjHZF6fubMkwhjPNRryi0BOyas9zlp6JUsNN0RQ4KRma8satN1JwEOAxlhMgJ7OwgRBbwuqCCiwhdylowbq0xpBsXZbhexgZnq4yOUb'] \n",
            "Min: 1 --> ['M' '9' '1']\n",
            "[[0.00094165 0.00094165 0.00105936 ... 0.00141248 0.00105936 0.00094165]]\n",
            "[1 1 1 ... 1 1 1]\n",
            "['megzy123' 'intel1' 'schalke04' ... 'jenny1989' 'skyline123' 'hattrick9']\n",
            "[1 1 1 ... 1 1 1]\n",
            "['martin1' 'harley1' 'star69' 'dagger1' 'c00per' 'family1' 'michael1'\n",
            " 'ashley1' 'matti1' 'rocket1' 'florida1' 'scott1' 'front242' 'teddy1'\n",
            " 'viper1' 'amanda1' 'phoenix1' 'daniel1' 'rasta1' 'david1' 'rocky1'\n",
            " 'hello123' 'randy1' 'justin1' 'seven7' 'saturn5' 'vampire' 'lucky1'\n",
            " 'master1' 'babylon5' 'xxx123' 'mickey1' 'montana3' '1234qwer' 'happy123'\n",
            " 'cindy1' 'terry1' 'chester1' 'steph1' 'roger1' 'carol1' 'Golden' '654321'\n",
            " 'trustno1' 'pussy1' 'parola' 'simba1' 'peter1' 'william1' 'billy1'\n",
            " 'rambo1' 'Lindsay' 'james1' 'apollo13' 'h2opolo' 'happy1' 'eagle1'\n",
            " 'joker1' 'qqq111' 'catch22' 'qwerty' 'andrew1' 'vincent1' '!@#$%^&'\n",
            " 'ncc1701d' 'jeffrey1' 'andre1' 'wombat1' 'x-files' 'magic1' 'money1'\n",
            " 'smile1' 'kelly1' 'asdf1234' 'Service' 'newyork1' 'scooter1' 'guitar1'\n",
            " 'hello8' '369' 'jordan23' 'tyler1' 'enigma' 'water1' 'alpha1' 'wendy1'\n",
            " 'kevin1' 'julie1' 'mariah1' 'larry1' 'mouse1' 'germany1' 'robert1'\n",
            " '121212' 'jesse1' 'qwerty12' 'soccer1' 'honda1' 'dragon1' 'calvin1'\n",
            " 'hello1' 'abcde' '12345' 'george1' 'tiger2' 'batman1' 'barney1' 'spike1'\n",
            " 'system5' 'route66' 'number9' 'glider1' 'nirvana1' 'keith1' 'susan1'\n",
            " 'karen1' '123456' 'nexus6' 'heather1' 'mustang1' 'charlie1' 'joshua'\n",
            " 'angel1' 'chris1' 'victor1' 'piano1' 'a1b2c3d4' 'pedro1' 'jesus1'\n",
            " 'abcd1234' 'chris123' 'hawkeye1' 'zxcvbnm' 'bubba1' 'maria1' 'sarah1'\n",
            " 'monkey1' 'student2' 'christ1' '1' 'andrea1' 'shadow1' 'porsche911'\n",
            " '1234' 'secret' 'jason1' 'ernie1' 'molly1' 'apple1' 'mulder1' 'colt45'\n",
            " 'chelsea1' 'richard1' 'apple2' 'sunny1' 'ricardo1' 'sherry' 'steven1'\n",
            " '123123' 'rabbit1' '!@#$%^' '123' 'alice1' 'wayne1' '000000' 'steve1'\n",
            " 'gustavo' 'skipper1' 'ncc1701e' 'Knight' 'admin1' 'safety1' 'fuckyou'\n",
            " '111111']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password structure feature"
      ],
      "metadata": {
        "id": "uxIR4HNJgG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define macros\n",
        "numbers = 0\n",
        "lower_case = 1\n",
        "upper_case = 2\n",
        "special_char = 3\n",
        "\n",
        "# Counts numbers, lowercases, uppercases and other characters in a password\n",
        "def calculate_password_structure(passwords):\n",
        "  passwords_structure = []\n",
        "  for password in passwords:\n",
        "    counts = np.array([0, 0, 0, 0])\n",
        "    for character in password:\n",
        "      if character.isnumeric():\n",
        "        counts[numbers] += 1\n",
        "      elif character.islower():\n",
        "        counts[lower_case] += 1\n",
        "      elif character.isupper():\n",
        "        counts[upper_case] += 1\n",
        "      else:\n",
        "        counts[special_char] += 1\n",
        "\n",
        "    passwords_structure.append(counts / len(password))\n",
        "  return np.array(passwords_structure)\n",
        "\n",
        "feat_structure = calculate_password_structure(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_structure[i])"
      ],
      "metadata": {
        "id": "wLptS-t0gAEK",
        "outputId": "a878a8f9-3b35-4cce-a40e-33d19cfcd330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 [0.5 0.5 0.  0. ]\n",
            "kino3434 [0.5 0.5 0.  0. ]\n",
            "visi7k1yr [0.22222222 0.77777778 0.         0.        ]\n",
            "megzy123 [0.375 0.625 0.    0.   ]\n",
            "lamborghin1 [0.09090909 0.90909091 0.         0.        ]\n",
            "AVYq1lDE4MgAZfNt [0.125  0.3125 0.5625 0.    ]\n",
            "u6c8vhow [0.25 0.75 0.   0.  ]\n",
            "v1118714 [0.875 0.125 0.    0.   ]\n",
            "universe2908 [0.33333333 0.66666667 0.         0.        ]\n",
            "as326159 [0.75 0.25 0.   0.  ]\n",
            "asv5o9yu [0.25 0.75 0.   0.  ]\n",
            "612035180tok [0.75 0.25 0.   0.  ]\n",
            "jytifok873 [0.3 0.7 0.  0. ]\n",
            "WUt9IZzE0OQ7PkNE [0.1875 0.1875 0.625  0.    ]\n",
            "jerusalem393 [0.25 0.75 0.   0.  ]\n",
            "g067057895 [0.9 0.1 0.  0. ]\n",
            "52558000aaa [0.72727273 0.27272727 0.         0.        ]\n",
            "idofo673 [0.375 0.625 0.    0.   ]\n",
            "6975038lp [0.77777778 0.22222222 0.         0.        ]\n",
            "sbl571017 [0.66666667 0.33333333 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password score feature by characters appearance frequency"
      ],
      "metadata": {
        "id": "Klkyyk59omsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate passwords score with character weights\n",
        "def calculate_password_scores(passwords):\n",
        "  scores = []\n",
        "  for password in passwords:\n",
        "    score = 0\n",
        "    for character in password:\n",
        "      # If weight does not exists in the dictionary, it is a very rare character so 1.001 points are given\n",
        "      try:\n",
        "        score += weights_dict[character]\n",
        "      except:\n",
        "        score += 1\n",
        "    scores.append(score)\n",
        "\n",
        "  return np.array(scores)\n",
        "\n",
        "feat_scores = calculate_password_scores(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_scores[i])"
      ],
      "metadata": {
        "id": "T3vr_4C-o06P",
        "outputId": "498eb537-feb5-4ee2-fe06-09bfeaed4134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 1284387\n",
            "kino3434 1613098\n",
            "visi7k1yr 1784524\n",
            "megzy123 1675490\n",
            "lamborghin1 2377776\n",
            "AVYq1lDE4MgAZfNt 1404851\n",
            "u6c8vhow 1147941\n",
            "v1118714 2106240\n",
            "universe2908 2555804\n",
            "as326159 1999635\n",
            "asv5o9yu 1590330\n",
            "612035180tok 2794891\n",
            "jytifok873 1629848\n",
            "WUt9IZzE0OQ7PkNE 1210915\n",
            "jerusalem393 2584905\n",
            "g067057895 1776684\n",
            "52558000aaa 2928381\n",
            "idofo673 1475785\n",
            "6975038lp 1574155\n",
            "sbl571017 1958894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Create the feature vector and split dataset\n",
        "# Currently we are not including \"feat_score\"\n",
        "\n",
        "# divide \"feat_structure\" in its sub-features since hstack() needs the same\n",
        "# dimension for every array\n",
        "\n",
        "def create_feature_vector(features_list, n_feat):\n",
        "  # features_list --> [length, rockyou, jtr, structure]\n",
        "  # feat_structure has 4 sub-features (numbers, lowercase, uppercase, special)\n",
        "  feature_vector = np.zeros(shape=(n_samples, n_feat))\n",
        "\n",
        "  for i, feature in enumerate(features_list):\n",
        "    # \"-1\" allows to let Python compute the remaining dimension\n",
        "    # in the case of \"feature = feat_struct\" the second dim = 4\n",
        "    feature = feature.reshape(n_samples, -1)\n",
        "    feature_vector[:, i:i+feature.shape[1]] = feature\n",
        "\n",
        "  return feature_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nFeature length: \", feat_length)\n",
        "print(\"\\nFeature rockyou: \", feat_rockyou)\n",
        "print(\"\\nFeature jtr: \", feat_jtr)\n",
        "print(\"\\nFeature structure --> numbers: \", feat_structure[:, 0])\n",
        "print(\"\\nFeature structure --> lowercase: \", feat_structure[:, 1])\n",
        "print(\"\\nFeature structure --> uppercase: \", feat_structure[:, 2])\n",
        "print(\"\\nFeature structure --> special: \", feat_structure[:, 3])\n",
        "\n",
        "feat_list = [feat_length, feat_rockyou, feat_jtr, feat_structure]\n",
        "feature_vector = create_feature_vector(feat_list, n_feat=7)\n",
        "print(\"\\n\")\n",
        "print(feature_vector.shape)\n",
        "print(feature_vector)\n",
        "\n",
        "# 80% of the dataset is used for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.20, random_state=42)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OzjdbbivBUF",
        "outputId": "78aca45f-dac9-4b3b-b3d8-5c2194210410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature length:  [[0.00094165 0.00094165 0.00105936 ... 0.00141248 0.00105936 0.00094165]]\n",
            "\n",
            "Feature rockyou:  [1 1 1 ... 1 1 1]\n",
            "\n",
            "Feature jtr:  [1 1 1 ... 1 1 1]\n",
            "\n",
            "Feature structure --> numbers:  [0.5        0.5        0.22222222 ... 0.5        0.22222222 0.25      ]\n",
            "\n",
            "Feature structure --> lowercase:  [0.5        0.5        0.77777778 ... 0.5        0.77777778 0.75      ]\n",
            "\n",
            "Feature structure --> uppercase:  [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Feature structure --> special:  [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "\n",
            "(669639, 7)\n",
            "[[9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.05935781e-03 1.00000000e+00 1.00000000e+00 ... 7.77777778e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.41247708e-03 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.05935781e-03 1.00000000e+00 1.00000000e+00 ... 7.77777778e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 7.50000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "(535711, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "# Note: SVC may be impractical beyond ten of thousands of samples.\n",
        "# Use \"LinearSVC\" or \"SGDClassifier\" instead\n",
        "\n",
        "clf = LinearSVC(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy: \", accuracy * 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "-qpWcgh-22Yl",
        "outputId": "cf2e4a24-bf08-4083-e9f3-27d6fd5edcd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  82.86168687653067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy: \", accuracy * 100)\n"
      ],
      "metadata": {
        "id": "KeD5AkYT23nx",
        "outputId": "0ee1d163-2e13-420b-c5b4-49cf0374c7a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  82.8594468669733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Neural Network"
      ],
      "metadata": {
        "id": "P2Ovy96LylJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Dataset Creation"
      ],
      "metadata": {
        "id": "I7XC8I8m-CAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "rEQlGb3Y-FES"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Trainloader creation"
      ],
      "metadata": {
        "id": "WqLB76e--R4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_DataLoader(trainset, testset):\n",
        "  # trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "  # No need to shuffle test data.\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "1_bBkaz_-V0t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.mid_n = int((n_input + n_output) / 2)\n",
        "\n",
        "    # Define Layers:\n",
        "    self.l1 = nn.Linear(self.n_input, self.mid_n) # layer 1\n",
        "    self.l2 = nn.Linear(self.mid_n, self.n_output) # layer 2\n",
        "    self.l3 = nn.Linear(self.n_output, self.n_output) # layer 3\n",
        "\n",
        "    # Define Activation functions:\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # Weights initialization\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='relu') # Using HE because more optimized for ReLU activated layers\n",
        "    nn.init.zeros_(self.l1.bias)\n",
        "    nn.init.kaiming_normal_(self.l2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.l2.bias)\n",
        "    nn.init.normal_(self.l3.weight) # Using normal distribution for LogSoftMax activated layer\n",
        "    nn.init.normal_(self.l3.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Layers: 3\n",
        "    Activation Functions:\n",
        "    RELU for first two layers\n",
        "    Log Softmax for last layer\n",
        "    '''\n",
        "    x = self.l1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def train_model(self, tr_loader, n_epochs, criterion, optimizer):\n",
        "    # losses --> {[*idx_first_epoch*]: loss_1, [*idx_second_epoch*]: loss_2, ...}\n",
        "    losses = {}\n",
        "    for e in range(n_epochs):\n",
        "      for features, labels in tr_loader:\n",
        "        optimizer.zero_grad() # set optimizer gradients to zero:\n",
        "        output = self(features) # Intial output\n",
        "        loss = criterion(output, labels.long()) # Loss Caluclation\n",
        "        loss.backward() # Pass loss function gradients to pervious layers:\n",
        "        optimizer.step() # Update Weights\n",
        "        losses[e] = loss.item()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "  def test_model(self, ts_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in ts_loader:\n",
        "      outputs = self(features)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "# Define macros.\n",
        "number_of_features = 8 # Rockyou, johntheripper , password len, numbers/len, lowercases/len, uppercases/len, special characters/len, password score\n",
        "number_of_outputs = 3 # Unsecure, intermediate, secure\n",
        "\n",
        "# Initialize NN\n",
        "NN = NeuralNetwork(number_of_features, number_of_outputs)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(NN.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4FDFyMMQypbx"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}