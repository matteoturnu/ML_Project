{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize as nrm\n",
        "import random\n",
        "from importlib.util import find_spec\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn # Contains Required functions and layers\n",
        "import torch.nn.functional as F # For neural network functions:\n",
        "import torch.optim as optim # Contains Optimization function available in PyTorch.\n",
        "project_folder = \"/content/password_strength_classifier\""
      ],
      "metadata": {
        "id": "KykGU56SuABu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of useful directories and download of password dictionaries"
      ],
      "metadata": {
        "id": "V8T3qe2zMghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, dest_folder_name):\n",
        "  local_filename = url.split('/')[-1]\n",
        "  path = os.path.join(\"/{}/{}\".format(dest_folder_name, local_filename))\n",
        "  \"\"\"with requests.get(url, stream=True) as r:\n",
        "      with open(path, 'wb') as f:\n",
        "          shutil.copyfileobj(r.raw, f)\"\"\"\n",
        "\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(requests.get(url, stream=True).content)\n",
        "\n",
        "  # return local_filename\n",
        "  return path\n",
        "\n",
        "def read_file(filepath):\n",
        "  with open(filepath, errors='replace', encoding='utf-8') as f:\n",
        "    data = {line.split('\\n')[0] for line in f.readlines()}\n",
        "  return data\n",
        "\n",
        "\n",
        "if os.path.exists(project_folder) is False:\n",
        "  dict_dir = project_folder + \"/dictionaries/\"\n",
        "  dataset_dir = project_folder + \"/dataset/\"\n",
        "\n",
        "  os.mkdir(project_folder)\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "  os.mkdir(dict_dir)\n",
        "\n",
        "  f_rockyou = download_file(\n",
        "    \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\",\n",
        "    dict_dir)\n",
        "\n",
        "  f_jtr = download_file(\n",
        "      \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Software/john-the-ripper.txt\",\n",
        "      dict_dir)\n",
        "\n",
        "\n",
        "data_rockyou = read_file(f_rockyou)\n",
        "data_jtr = read_file(f_jtr)\n",
        "\n",
        "print(\"\\nRock You\")\n",
        "import itertools\n",
        "print([val for i, val in enumerate(itertools.islice(data_rockyou, 5))])\n",
        "print(\"\\nJohn The Ripper\")\n",
        "print([val for i, val in enumerate(itertools.islice(data_jtr, 5))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynOVFhD_5q3r",
        "outputId": "4fac27eb-84b3-4fbd-fff9-5930ac0f57a5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rock You\n",
            "['', 'bitch(!!', 'sarahbutler', 'venice288', '35511435a']\n",
            "\n",
            "John The Ripper\n",
            "['israel', 'dixie', 'allison', 'home', 'jessie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "\n",
        "if find_spec(\"kaggle\") is None:\n",
        "  ! pip install -q kaggle\n",
        "\n",
        "if os.path.isdir(\"/root/.kaggle\") is False:\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! touch \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "  token = {\"username\":\"matteoturnu\",\"key\":\"79ea644685a3e574038b40e4019b0927\"}\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d bhavikbb/password-strength-classifier-dataset -p $dataset_dir"
      ],
      "metadata": {
        "id": "y0pIiUJswG7d"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"password-strength-classifier-dataset.zip\")\n",
        "\n",
        "pswd_df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "print(pswd_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jiwHeBuGRgP",
        "outputId": "3d759d70-8cf4-4895-c82b-9f7ee67969bf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669640 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "_VFws5SNT4xt",
        "outputId": "868644db-c6ce-41c4-a043-41c44086f4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669639 rows x 2 columns]\n",
            "[['kzde5577' 1]\n",
            " ['kino3434' 1]\n",
            " ['visi7k1yr' 1]\n",
            " ...\n",
            " ['184520socram' 1]\n",
            " ['marken22a' 1]\n",
            " ['fxx4pw4g' 1]]\n",
            "Len of passwords:  669639\n",
            "Len of UNIQUE passwords:  669639\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vectorizer = TfidfVectorizer(tokenizer=word_split)\\nX = vectorizer.fit_transform(passwords)\\n\\nprint(vectorizer.get_feature_names_out())'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "def word_split(inputs):\n",
        "    character=[]\n",
        "    for i in inputs:\n",
        "        character.append(i)\n",
        "    return character\n",
        "\n",
        "# unique values of strength feature\n",
        "pswd_df['strength'].unique()\n",
        "\n",
        "# number of missing values in dataset\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "# remove missing values\n",
        "pswd_df.dropna(inplace=True)\n",
        "pswd_df.isnull().sum()\n",
        "\n",
        "print(pswd_df)\n",
        "\n",
        "psw_array = np.array(pswd_df)\n",
        "print(psw_array)\n",
        "\n",
        "# ??? PROBLEM: it is shown that if shuffled then\n",
        "# there are several duplicates of a password\n",
        "#random.shuffle(psw_array)\n",
        "\n",
        "labels = np.array([p[1] for p in psw_array])\n",
        "passwords = np.array([p[0] for p in psw_array])\n",
        "\n",
        "print(\"Len of passwords: \", len(passwords))\n",
        "print(\"Len of UNIQUE passwords: \", len(np.unique(passwords)))\n",
        "\n",
        "n_samples = len(passwords)\n",
        "n_features = 8\n",
        "\n",
        "\"\"\"vectorizer = TfidfVectorizer(tokenizer=word_split)\n",
        "X = vectorizer.fit_transform(passwords)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Normalization function"
      ],
      "metadata": {
        "id": "Ze_Zidy-pz_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(value, min, max):\n",
        "  return (value - min) / (max - min)"
      ],
      "metadata": {
        "id": "nZ2CQlh4p2hY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Calculate characters weights"
      ],
      "metadata": {
        "id": "0XE_dLdSqh5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary containing occurrencies of all the characters in the dataset\n",
        "def calculate_occurrencies(passwords):\n",
        "  tot_occurrencies = 0\n",
        "  occurrencies_dict = dict()\n",
        "  for password in passwords:\n",
        "    for character in password:\n",
        "      tot_occurrencies += 1\n",
        "      if character in occurrencies_dict:\n",
        "        occurrencies_dict[character] += 1\n",
        "      else:\n",
        "        occurrencies_dict[character] = 1\n",
        "  return occurrencies_dict, tot_occurrencies\n",
        "\n",
        "# Converts occurrencies to weights\n",
        "def calculate_weights(occurrencies_dict, tot_occurrencies):\n",
        "  weights_dict = dict()\n",
        "  for key in occurrencies_dict:\n",
        "    weights_dict[key] = 1 - (occurrencies_dict[key] / tot_occurrencies)\n",
        "  return weights_dict\n",
        "\n",
        "def normalize_weights_dict(weights_dict):\n",
        "  maximum = max(weights_dict.values())\n",
        "  minimum = min(weights_dict.values())\n",
        "  normalized_weights_dict = dict()\n",
        "  for key in weights_dict:\n",
        "    normalized_weights_dict[key] = normalize(weights_dict[key], minimum, maximum)\n",
        "  return normalized_weights_dict\n",
        "\n",
        "occurrencies_dict, tot_occurrencies = calculate_occurrencies(passwords)\n",
        "weights_dict = calculate_weights(occurrencies_dict, tot_occurrencies)\n",
        "normalized_weights_dict = normalize_weights_dict(weights_dict)\n",
        "print(normalized_weights_dict)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9qsx0MWqnJa",
        "outputId": "6a6dbb06-4cc8-4cde-ec93-0fe0b2a69ab8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'k': 0.6657636716788613, 'z': 0.7784853687869094, 'd': 0.680859037257116, 'e': 0.2982357306788878, '5': 0.5838229999191711, '7': 0.6315560007935909, 'i': 0.37886109961275827, 'n': 0.4990337300469549, 'o': 0.3892096181212973, '3': 0.47419728562645974, '4': 0.5838548415145818, 'v': 0.8009630857935044, 's': 0.5261848135184723, '1': 0.055509698215155584, 'y': 0.6797029424083516, 'r': 0.5116895395460378, 'm': 0.6072412686671358, 'g': 0.6905658251504523, '2': 0.312214191064269, 'l': 0.5871467726093695, 'a': 0.0, 'b': 0.7437045491842426, 'h': 0.7130508902175275, 'A': 0.8941389420262521, 'V': 0.9695202451312972, 'Y': 0.9480688072383304, 'q': 0.8649475470949451, 'D': 0.9276583445799487, 'E': 0.9401794396369086, 'M': 0.9007007600343888, 'Z': 0.9692263227121206, 'f': 0.8096288984522538, 'N': 0.9141256665303191, 't': 0.6096514325043904, 'u': 0.6063570520561109, '6': 0.6317788919614675, 'c': 0.6825099015114968, '8': 0.6026805724628994, 'w': 0.761756284428726, '9': 0.5184987422569818, '0': 0.3870076493309591, 'j': 0.7666990797778928, 'W': 0.9690891589165042, 'U': 0.9483112932341508, 'I': 0.9435962877598458, 'O': 0.9308841431304217, 'Q': 0.920391112765786, 'P': 0.967134574828975, 'p': 0.7276710812181133, '@': 0.9866583715228365, '-': 0.9922967832640575, 'H': 0.9673648140573302, 'x': 0.8298017738218002, '.': 0.9848703434728412, 'T': 0.9270607023276201, '>': 0.9997722101251383, 'G': 0.9662528075714416, 'J': 0.9671468215964403, '&': 0.998376078634045, '?': 0.9987753232534273, '<': 0.9997134256413018, '!': 0.9946579600314491, 'S': 0.9615353527436432, 'R': 0.9631078376862421, 'F': 0.9682784229102724, 'B': 0.9670757903451386, 'K': 0.9667671718050016, 'X': 0.9695716815546533, 'C': 0.9661450360177425, 'L': 0.9644696782284313, ';': 0.9991133340354816, '_': 0.9928331916790563, '%': 0.9990006637747975, '±': 0.9996766853389043, '\"': 0.9999804051720546, '~': 0.9997746594786316, '+': 0.9983074967362368, '^': 0.9991745678728097, '/': 0.9979596885402106, '$': 0.9972101863713069, ')': 0.9991084353284948, ' ': 0.9972836669761019, '(': 0.9992309030031526, '#': 0.9970436303337731, 'Ú': 0.999938766162672, '*': 0.9954050128468594, '`': 0.9999706077580827, '{': 0.9999069245672613, '}': 0.9999118232742463, '[': 0.9996350463295218, ']': 0.9996791346923978, 'þ': 0.9999436648696588, 'Þ': 0.9999828545255479, 'Ó': 0.9999608103441091, 'Ô': 0.9999902025860282, '=': 0.9994146045151384, '\\\\': 0.9999044752137679, '\\x1c': 0.9999902025860282, '³': 0.9999436648696588, '¿': 0.9999902025860282, '\\x16': 0.9999951012930132, 'Ò': 0.9999975506465066, '·': 0.9999926519395216, '\\x1e': 0.9999975506465066, '\\x19': 1.0, '\\x05': 0.9999951012930132, '\\x1b': 0.9999975506465066, 'Å': 0.9999706077580827, '‚': 1.0, 'Ä': 0.999882431032329, 'à': 0.9999975506465066, 'õ': 0.9999706077580827, 'ß': 0.9999779558185611, '´': 0.9999975506465066, '«': 1.0, 'ð': 0.9999902025860282, 'Ð': 0.9999534622836307, 'å': 0.9999877532325347, 'â': 0.9999975506465066, '°': 0.9999730571115761, '|': 0.9999118232742463, '\\x7f': 0.9999975506465066, '²': 0.9999975506465066, '¾': 0.9999877532325347, 'Ÿ': 0.9999706077580827, '\\x08': 1.0, 'ê': 1.0, 'á': 0.9999902025860282, '\\x10': 0.9999926519395216, '\\x17': 0.9999926519395216, 'º': 0.9999779558185611, '¡': 0.9999951012930132, '÷': 0.9999608103441091, 'Õ': 0.9999975506465066, 'í': 0.9999975506465066, 'ú': 0.9999877532325347, 'µ': 0.9999926519395216, 'Ý': 0.9999926519395216, 'Ü': 0.9999951012930132, 'Û': 0.9999975506465066, 'Ö': 0.9999975506465066, '×': 1.0, '¨': 0.9999926519395216, '\\xa0': 0.9999975506465066, 'æ': 0.9999951012930132, 'è': 1.0, 'ù': 0.9999975506465066, 'É': 1.0, '\\x06': 0.9999975506465066, 'Ñ': 0.9999926519395216, '\\x81': 0.9999975506465066, '\\x11': 0.9999975506465066, 'Â': 0.9999951012930132, 'ý': 1.0, '—': 0.9999975506465066, '›': 0.9999975506465066, '‹': 1.0, 'œ': 1.0, '™': 1.0, 'ó': 1.0, '¦': 1.0, '\\x0f': 1.0, 'ä': 1.0, 'ï': 1.0, 'Ï': 1.0, 'Ç': 0.9999853038790414, 'ò': 1.0, 'ö': 1.0, '\\x12': 0.9999926519395216, '\\x8d': 0.9999975506465066, 'î': 1.0, '¹': 1.0, '¶': 1.0, 'ñ': 1.0, '¼': 0.9999975506465066, 'Ù': 1.0, '…': 1.0, '\\x13': 1.0, '\\x1d': 1.0, '\\x04': 1.0, '\\x0e': 1.0, '\\x02': 1.0, '¯': 1.0, '\\x01': 1.0, 'Ê': 1.0, 'Æ': 0.9999975506465066, '‡': 1.0, '¤': 1.0, 'À': 1.0, '¢': 1.0, 'û': 1.0, 'ƒ': 1.0, 'é': 1.0, 'Á': 1.0, '§': 1.0, 'Í': 0.9999975506465066, 'Ã': 1.0, '»': 1.0, '\\x18': 1.0, '½': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LENGTH feature\n",
        "# lengths = np.array([len(p) for p in passwords]).reshape(-1, 1)\n",
        "lengths = np.array([len(p) for p in passwords])\n",
        "max_len = np.max(lengths)\n",
        "min_len = np.min(lengths)\n",
        "\n",
        "print(\"Lengths shape: \", lengths.shape)\n",
        "\n",
        "print(lengths)\n",
        "print(f\"Max: {max_len} --> {passwords[lengths == max_len]} \\nMin: {min_len} --> {passwords[lengths == min_len]}\")\n",
        "\n",
        "# normalize() from sklearn accept 2D arrays only --> reshape so that we have 1 row (1 \"sample\")\n",
        "# and make sklearn compute the remaining number of columns for us\n",
        "feat_length = nrm(lengths.reshape(1, -1))\n",
        "\n",
        "#feat_length = feat_length.flatten()\n",
        "print(feat_length)\n",
        "\n",
        "\n",
        "# ROCKYOU feature\n",
        "\n",
        "# numpy array of 1 and 0 (0 if found, otherwise is 1)\n",
        "# int() used to convert boolean into number\n",
        "feat_rockyou = np.array([int(p not in data_rockyou) for p in passwords])\n",
        "print(feat_rockyou)\n",
        "\n",
        "# passwords found in rockyou file\n",
        "print(passwords[feat_rockyou == 0])\n",
        "\n",
        "\n",
        "# JTR feature\n",
        "feat_jtr = np.array([int(p not in data_jtr) for p in passwords])\n",
        "print(feat_jtr)\n",
        "print(passwords[feat_jtr == 0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awONOEU9HOhF",
        "outputId": "f2634f6d-8f9c-4c87-8d9a-a903934f7555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths shape:  (669639,)\n",
            "[ 8  8  9 ... 12  9  8]\n",
            "Max: 220 --> ['In0LnUoff8wfayJGqzelyDqg4AMl9gBhgl3T2iZeONzh5gPqTyP8IVLsQ960aZwlZcdSjE1XCi8taVT5dWSB3wNJwMqpzmlSIKh21A8TNxpSJ5nu2hULRgjHZF6fubMkwhjPNRryi0BOyas9zlp6JUsNN0RQ4KRma8satN1JwEOAxlhMgJ7OwgRBbwuqCCiwhdylowbq0xpBsXZbhexgZnq4yOUb'] \n",
            "Min: 1 --> ['M' '9' '1']\n",
            "[[0.00094165 0.00094165 0.00105936 ... 0.00141248 0.00105936 0.00094165]]\n",
            "[1 1 1 ... 1 1 1]\n",
            "['megzy123' 'intel1' 'schalke04' ... 'jenny1989' 'skyline123' 'hattrick9']\n",
            "[1 1 1 ... 1 1 1]\n",
            "['martin1' 'harley1' 'star69' 'dagger1' 'c00per' 'family1' 'michael1'\n",
            " 'ashley1' 'matti1' 'rocket1' 'florida1' 'scott1' 'front242' 'teddy1'\n",
            " 'viper1' 'amanda1' 'phoenix1' 'daniel1' 'rasta1' 'david1' 'rocky1'\n",
            " 'hello123' 'randy1' 'justin1' 'seven7' 'saturn5' 'vampire' 'lucky1'\n",
            " 'master1' 'babylon5' 'xxx123' 'mickey1' 'montana3' '1234qwer' 'happy123'\n",
            " 'cindy1' 'terry1' 'chester1' 'steph1' 'roger1' 'carol1' 'Golden' '654321'\n",
            " 'trustno1' 'pussy1' 'parola' 'simba1' 'peter1' 'william1' 'billy1'\n",
            " 'rambo1' 'Lindsay' 'james1' 'apollo13' 'h2opolo' 'happy1' 'eagle1'\n",
            " 'joker1' 'qqq111' 'catch22' 'qwerty' 'andrew1' 'vincent1' '!@#$%^&'\n",
            " 'ncc1701d' 'jeffrey1' 'andre1' 'wombat1' 'x-files' 'magic1' 'money1'\n",
            " 'smile1' 'kelly1' 'asdf1234' 'Service' 'newyork1' 'scooter1' 'guitar1'\n",
            " 'hello8' '369' 'jordan23' 'tyler1' 'enigma' 'water1' 'alpha1' 'wendy1'\n",
            " 'kevin1' 'julie1' 'mariah1' 'larry1' 'mouse1' 'germany1' 'robert1'\n",
            " '121212' 'jesse1' 'qwerty12' 'soccer1' 'honda1' 'dragon1' 'calvin1'\n",
            " 'hello1' 'abcde' '12345' 'george1' 'tiger2' 'batman1' 'barney1' 'spike1'\n",
            " 'system5' 'route66' 'number9' 'glider1' 'nirvana1' 'keith1' 'susan1'\n",
            " 'karen1' '123456' 'nexus6' 'heather1' 'mustang1' 'charlie1' 'joshua'\n",
            " 'angel1' 'chris1' 'victor1' 'piano1' 'a1b2c3d4' 'pedro1' 'jesus1'\n",
            " 'abcd1234' 'chris123' 'hawkeye1' 'zxcvbnm' 'bubba1' 'maria1' 'sarah1'\n",
            " 'monkey1' 'student2' 'christ1' '1' 'andrea1' 'shadow1' 'porsche911'\n",
            " '1234' 'secret' 'jason1' 'ernie1' 'molly1' 'apple1' 'mulder1' 'colt45'\n",
            " 'chelsea1' 'richard1' 'apple2' 'sunny1' 'ricardo1' 'sherry' 'steven1'\n",
            " '123123' 'rabbit1' '!@#$%^' '123' 'alice1' 'wayne1' '000000' 'steve1'\n",
            " 'gustavo' 'skipper1' 'ncc1701e' 'Knight' 'admin1' 'safety1' 'fuckyou'\n",
            " '111111']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password structure feature"
      ],
      "metadata": {
        "id": "uxIR4HNJgG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define macros\n",
        "numbers = 0\n",
        "lower_case = 1\n",
        "upper_case = 2\n",
        "special_char = 3\n",
        "\n",
        "# Counts numbers, lowercases, uppercases and other characters in a password\n",
        "def calculate_password_structure(passwords):\n",
        "  passwords_structure = []\n",
        "  for password in passwords:\n",
        "    counts = np.array([0, 0, 0, 0])\n",
        "    for character in password:\n",
        "      if character.isnumeric():\n",
        "        counts[numbers] += 1\n",
        "      elif character.islower():\n",
        "        counts[lower_case] += 1\n",
        "      elif character.isupper():\n",
        "        counts[upper_case] += 1\n",
        "      else:\n",
        "        counts[special_char] += 1\n",
        "\n",
        "    passwords_structure.append(counts / len(password))\n",
        "  return np.array(passwords_structure)\n",
        "\n",
        "feat_structure = calculate_password_structure(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_structure[i])"
      ],
      "metadata": {
        "id": "wLptS-t0gAEK",
        "outputId": "90a2d30a-b6d8-4478-f6e0-21c3343141da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 [0.5 0.5 0.  0. ]\n",
            "kino3434 [0.5 0.5 0.  0. ]\n",
            "visi7k1yr [0.22222222 0.77777778 0.         0.        ]\n",
            "megzy123 [0.375 0.625 0.    0.   ]\n",
            "lamborghin1 [0.09090909 0.90909091 0.         0.        ]\n",
            "AVYq1lDE4MgAZfNt [0.125  0.3125 0.5625 0.    ]\n",
            "u6c8vhow [0.25 0.75 0.   0.  ]\n",
            "v1118714 [0.875 0.125 0.    0.   ]\n",
            "universe2908 [0.33333333 0.66666667 0.         0.        ]\n",
            "as326159 [0.75 0.25 0.   0.  ]\n",
            "asv5o9yu [0.25 0.75 0.   0.  ]\n",
            "612035180tok [0.75 0.25 0.   0.  ]\n",
            "jytifok873 [0.3 0.7 0.  0. ]\n",
            "WUt9IZzE0OQ7PkNE [0.1875 0.1875 0.625  0.    ]\n",
            "jerusalem393 [0.25 0.75 0.   0.  ]\n",
            "g067057895 [0.9 0.1 0.  0. ]\n",
            "52558000aaa [0.72727273 0.27272727 0.         0.        ]\n",
            "idofo673 [0.375 0.625 0.    0.   ]\n",
            "6975038lp [0.77777778 0.22222222 0.         0.        ]\n",
            "sbl571017 [0.66666667 0.33333333 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Password score feature by characters appearance frequency"
      ],
      "metadata": {
        "id": "Klkyyk59omsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate passwords score with character weights\n",
        "def calculate_password_scores(passwords):\n",
        "  scores = []\n",
        "  for password in passwords:\n",
        "    score = 0\n",
        "    for character in password:\n",
        "      # If weight does not exists in the dictionary, it is a very rare character so 1.001 points are given\n",
        "      try:\n",
        "        score += normalized_weights_dict[character]\n",
        "      except:\n",
        "        score += 1\n",
        "    normalized_score = score / len(password)\n",
        "    scores.append(normalized_score)\n",
        "\n",
        "  return np.array(scores)\n",
        "\n",
        "feat_scores = calculate_password_scores(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_scores[i])"
      ],
      "metadata": {
        "id": "T3vr_4C-o06P",
        "outputId": "efa94477-f591-421d-d475-47084b9bda4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 0.6067627262284123\n",
            "kino3434 0.5061215467177443\n",
            "visi7k1yr 0.5143435501310545\n",
            "megzy123 0.48701903882470265\n",
            "lamborghin1 0.47054663557917564\n",
            "AVYq1lDE4MgAZfNt 0.7849414053410604\n",
            "u6c8vhow 0.6485382870691287\n",
            "v1118714 0.35513666167814983\n",
            "universe2908 0.47833016142056034\n",
            "as326159 0.38777582782024717\n",
            "asv5o9yu 0.5130924067592362\n",
            "612035180tok 0.42952944653592046\n",
            "jytifok873 0.6007950601438756\n",
            "WUt9IZzE0OQ7PkNE 0.8146300190314772\n",
            "jerusalem393 0.47239027508689135\n",
            "g067057895 0.5648297331919243\n",
            "52558000aaa 0.3479442464797782\n",
            "idofo673 0.5481625562432801\n",
            "6975038lp 0.5715955551310014\n",
            "sbl571017 0.4668886869533008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Create the feature vector and split dataset\n",
        "# Currently we are not including \"feat_score\"\n",
        "\n",
        "# divide \"feat_structure\" in its sub-features since hstack() needs the same\n",
        "# dimension for every array\n",
        "\n",
        "def create_feature_vector(features_list, n_feat):\n",
        "  # features_list --> [length, rockyou, jtr, structure]\n",
        "  # feat_structure has 4 sub-features (numbers, lowercase, uppercase, special)\n",
        "  feature_vector = np.zeros(shape=(n_samples, n_feat))\n",
        "\n",
        "  for i, feature in enumerate(features_list):\n",
        "    # \"-1\" allows to let Python compute the remaining dimension\n",
        "    # in the case of \"feature = feat_struct\" the second dim = 4\n",
        "    feature = feature.reshape(n_samples, -1)\n",
        "    feature_vector[:, i:i+feature.shape[1]] = feature\n",
        "\n",
        "  return feature_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nFeature length: \", feat_length)\n",
        "print(\"\\nFeature rockyou: \", feat_rockyou)\n",
        "print(\"\\nFeature jtr: \", feat_jtr)\n",
        "print(\"\\nFeature structure --> numbers: \", feat_structure[:, 0])\n",
        "print(\"\\nFeature structure --> lowercase: \", feat_structure[:, 1])\n",
        "print(\"\\nFeature structure --> uppercase: \", feat_structure[:, 2])\n",
        "print(\"\\nFeature structure --> special: \", feat_structure[:, 3])\n",
        "\n",
        "feat_list = [feat_length, feat_rockyou, feat_jtr, feat_structure]\n",
        "feature_vector = create_feature_vector(feat_list, n_feat=7)\n",
        "print(\"\\n\")\n",
        "print(feature_vector.shape)\n",
        "print(feature_vector)\n",
        "\n",
        "# 80% of the dataset is used for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.20, random_state=42)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OzjdbbivBUF",
        "outputId": "ca8a1ad5-6478-402f-d6cd-334ec175aa36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature length:  [[0.00094165 0.00094165 0.00105936 ... 0.00141248 0.00105936 0.00094165]]\n",
            "\n",
            "Feature rockyou:  [1 1 1 ... 1 1 1]\n",
            "\n",
            "Feature jtr:  [1 1 1 ... 1 1 1]\n",
            "\n",
            "Feature structure --> numbers:  [0.5        0.5        0.22222222 ... 0.5        0.22222222 0.25      ]\n",
            "\n",
            "Feature structure --> lowercase:  [0.5        0.5        0.77777778 ... 0.5        0.77777778 0.75      ]\n",
            "\n",
            "Feature structure --> uppercase:  [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Feature structure --> special:  [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "\n",
            "(669639, 7)\n",
            "[[9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.05935781e-03 1.00000000e+00 1.00000000e+00 ... 7.77777778e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.41247708e-03 1.00000000e+00 1.00000000e+00 ... 5.00000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.05935781e-03 1.00000000e+00 1.00000000e+00 ... 7.77777778e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.41651385e-04 1.00000000e+00 1.00000000e+00 ... 7.50000000e-01\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "(535711, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "# Note: SVC may be impractical beyond ten of thousands of samples.\n",
        "# Use \"LinearSVC\" or \"SGDClassifier\" instead\n",
        "\n",
        "clf = LinearSVC(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy: \", accuracy * 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "-qpWcgh-22Yl",
        "outputId": "3a5ad197-d9e0-4a37-8c2b-34e56f688e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  82.86168687653067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy: \", accuracy * 100)\n"
      ],
      "metadata": {
        "id": "KeD5AkYT23nx",
        "outputId": "c32e31c6-321b-4d8c-b9ae-027ec7960d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  82.8594468669733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Dataset Creation"
      ],
      "metadata": {
        "id": "I7XC8I8m-CAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "trainset = CustomDataset(X_train, y_train)\n",
        "testset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "rEQlGb3Y-FES"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Trainloader creation"
      ],
      "metadata": {
        "id": "WqLB76e--R4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_DataLoader(trainset, testset):\n",
        "  # trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "  # No need to shuffle test data.\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return trainloader, testloader\n",
        "\n",
        "trainloader, testloader = create_DataLoader(trainset, testset)"
      ],
      "metadata": {
        "id": "1_bBkaz_-V0t"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Neural Network"
      ],
      "metadata": {
        "id": "Y7Dzs4JiyXOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.mid_n = int((n_input + n_output) / 2)\n",
        "\n",
        "    # Define Layers:\n",
        "    self.l1 = nn.Linear(self.n_input, self.mid_n) # layer 1\n",
        "    self.l2 = nn.Linear(self.mid_n, self.n_output) # layer 2\n",
        "    self.l3 = nn.Linear(self.n_output, self.n_output) # layer 3\n",
        "\n",
        "    # Define Activation functions:\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # Weights initialization\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='relu') # Using HE because more optimized for ReLU activated layers\n",
        "    nn.init.zeros_(self.l1.bias)\n",
        "    nn.init.kaiming_normal_(self.l2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.l2.bias)\n",
        "    nn.init.normal_(self.l3.weight) # Using normal distribution for LogSoftMax activated layer\n",
        "    nn.init.normal_(self.l3.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Layers: 3\n",
        "    Activation Functions:\n",
        "    RELU for first two layers\n",
        "    Log Softmax for last layer\n",
        "    '''\n",
        "    x = self.l1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def train_model(self, tr_loader, n_epochs, criterion, optimizer):\n",
        "    # losses --> {[*idx_first_epoch*]: loss_1, [*idx_second_epoch*]: loss_2, ...}\n",
        "    losses = {}\n",
        "    for e in range(n_epochs):\n",
        "      for features, labels in tr_loader:\n",
        "        optimizer.zero_grad() # set optimizer gradients to zero:\n",
        "        output = self(features) # Intial output\n",
        "        loss = criterion(output, labels.long()) # Loss Caluclation\n",
        "        loss.backward() # Pass loss function gradients to pervious layers:\n",
        "        optimizer.step() # Update Weights\n",
        "        losses[e] = loss.item()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "  def test_model(self, ts_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in ts_loader:\n",
        "      outputs = self(features)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "# Define macros.\n",
        "number_of_features = 8 # Rockyou, johntheripper , password len, numbers/len, lowercases/len, uppercases/len, special characters/len, password score\n",
        "number_of_outputs = 3 # Unsecure, intermediate, secure\n",
        "\n",
        "# Initialize NN\n",
        "NN = NeuralNetwork(number_of_features, number_of_outputs)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(NN.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4FDFyMMQypbx"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel - Network training"
      ],
      "metadata": {
        "id": "ObgSGLWrzMNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Train the model\n",
        "n_epochs = 5\n",
        "losses = NN.train_model(trainloader, n_epochs, criterion, optimizer)\n",
        "print(losses)\n",
        "\n",
        "# Test the models\n",
        "accuracy = NN.test_model(testloader)\n",
        "print(f\"Accuracy of the model on the test images: {accuracy * 100} %\")\n",
        "'''"
      ],
      "metadata": {
        "id": "SRgndDBlzQgZ",
        "outputId": "f08110aa-5b4e-4a8e-964b-044832396d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-90f903669f1a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-b6cd0dfe7242>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, tr_loader, n_epochs, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set optimizer gradients to zero:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Intial output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Loss Caluclation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass loss function gradients to pervious layers:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-b6cd0dfe7242>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mLog\u001b[0m \u001b[0mSoftmax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     '''\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x7 and 8x5)"
          ]
        }
      ]
    }
  ]
}