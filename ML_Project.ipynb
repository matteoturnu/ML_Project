{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize as nrm\n",
        "import random\n",
        "from importlib.util import find_spec\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn # Contains Required functions and layers\n",
        "import torch.nn.functional as F # For neural network functions:\n",
        "import torch.optim as optim # Contains Optimization function available in PyTorch.\n",
        "project_folder = \"/content/password_strength_classifier\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KykGU56SuABu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of useful directories and download of password dictionaries"
      ],
      "metadata": {
        "id": "V8T3qe2zMghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def download_file(url, dest_folder_name):\n",
        "  local_filename = url.split('/')[-1]\n",
        "  path = os.path.join(\"/{}/{}\".format(dest_folder_name, local_filename))\n",
        "\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(requests.get(url, stream=True).content)\n",
        "  return path\n",
        "\n",
        "\n",
        "def read_file(filepath):\n",
        "  with open(filepath, errors='replace', encoding='utf-8') as f:\n",
        "    data = {line.split('\\n')[0] for line in f.readlines()}\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(project_folder) is False:\n",
        "  dict_dir = project_folder + \"/dictionaries/\"\n",
        "  dataset_dir = project_folder + \"/dataset/\"\n",
        "\n",
        "  os.mkdir(project_folder)\n",
        "  os.mkdir(dataset_dir)\n",
        "  os.mkdir(dict_dir)\n",
        "\n",
        "  f_rockyou = download_file(\n",
        "    \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\",\n",
        "    dict_dir)\n",
        "\n",
        "  f_jtr = download_file(\n",
        "      \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Software/john-the-ripper.txt\",\n",
        "      dict_dir)\n",
        "\n",
        "\n",
        "data_rockyou = read_file(f_rockyou)\n",
        "data_jtr = read_file(f_jtr)\n"
      ],
      "metadata": {
        "id": "ynOVFhD_5q3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "\n",
        "if find_spec(\"kaggle\") is None:\n",
        "  ! pip install -q kaggle\n",
        "\n",
        "if os.path.isdir(\"/root/.kaggle\") is False:\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! touch \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "  token = {\"username\":\"matteoturnu\",\"key\":\"79ea644685a3e574038b40e4019b0927\"}\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d bhavikbb/password-strength-classifier-dataset -p $dataset_dir"
      ],
      "metadata": {
        "id": "y0pIiUJswG7d",
        "outputId": "2bdd20ae-c9f1-4dd5-8a26-3d528ca20ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading password-strength-classifier-dataset.zip to /content/password_strength_classifier/dataset\n",
            "\r  0% 0.00/5.01M [00:00<?, ?B/s]\n",
            "\r100% 5.01M/5.01M [00:00<00:00, 118MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read password dataset\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"password-strength-classifier-dataset.zip\")\n",
        "\n",
        "pswd_df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "print(pswd_df)"
      ],
      "metadata": {
        "id": "8jiwHeBuGRgP",
        "outputId": "c2bdc1ca-d84f-43cc-938b-160f142ecfca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669640 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VFws5SNT4xt",
        "outputId": "8bc850e0-51bb-43d3-c8cb-30c8f63d8fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            password  strength\n",
            "0           kzde5577         1\n",
            "1           kino3434         1\n",
            "2          visi7k1yr         1\n",
            "3           megzy123         1\n",
            "4        lamborghin1         1\n",
            "...              ...       ...\n",
            "669635    10redtux10         1\n",
            "669636     infrared1         1\n",
            "669637  184520socram         1\n",
            "669638     marken22a         1\n",
            "669639      fxx4pw4g         1\n",
            "\n",
            "[669639 rows x 2 columns]\n",
            "[['kzde5577' 1]\n",
            " ['kino3434' 1]\n",
            " ['visi7k1yr' 1]\n",
            " ...\n",
            " ['184520socram' 1]\n",
            " ['marken22a' 1]\n",
            " ['fxx4pw4g' 1]]\n",
            "Len of UNIQUE passwords:  669639\n",
            "Number of samples:  669639\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# remove missing values\n",
        "pswd_df.dropna(inplace=True)\n",
        "print(pswd_df)\n",
        "psw_array = np.array(pswd_df)\n",
        "print(psw_array)\n",
        "\n",
        "# divide into X (passwords) and y (labels) arrays\n",
        "labels = np.array([p[1] for p in psw_array])\n",
        "passwords = np.array([p[0] for p in psw_array])\n",
        "\n",
        "\n",
        "print(\"Len of UNIQUE passwords: \", len(np.unique(passwords)))\n",
        "\n",
        "n_samples = len(passwords)\n",
        "print(\"Number of samples: \", n_samples)\n",
        "n_features = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(value, min, max):\n",
        "  return (value - min) / (max - min)\n",
        "\n",
        "# LENGTH feature\n",
        "lengths = np.array([len(p) for p in passwords])\n",
        "max_len = np.max(lengths)\n",
        "min_len = np.min(lengths)\n",
        "print(\"Lengths shape: \", lengths.shape)\n",
        "print(\"Lenghts:\", lengths)\n",
        "print(f\"Max: {max_len} --> {passwords[lengths == max_len]} \\nMin: {min_len} --> {passwords[lengths == min_len]}\")\n",
        "\n",
        "feat_length = normalize(lengths, min_len, max_len)\n",
        "print(feat_length)\n",
        "\n",
        "\n",
        "\n",
        "# ROCKYOU feature\n",
        "# numpy array of 1 and 0 (0 if rockyou file contains the current password, otherwise is 1)\n",
        "# int() used to convert boolean into number\n",
        "feat_rockyou = np.array([int(p not in data_rockyou) for p in passwords])\n",
        "print(\"Rockyou feature: \", feat_rockyou)\n",
        "\n",
        "\n",
        "# JTR feature\n",
        "feat_jtr = np.array([int(p not in data_jtr) for p in passwords])\n",
        "print(\"JTR feature: \", feat_jtr)\n"
      ],
      "metadata": {
        "id": "Mb6BzStWQbxV",
        "outputId": "16a3aa33-67f1-494f-987d-93da71ef007d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths shape:  (669639,)\n",
            "Lenghts: [ 8  8  9 ... 12  9  8]\n",
            "Max: 220 --> ['In0LnUoff8wfayJGqzelyDqg4AMl9gBhgl3T2iZeONzh5gPqTyP8IVLsQ960aZwlZcdSjE1XCi8taVT5dWSB3wNJwMqpzmlSIKh21A8TNxpSJ5nu2hULRgjHZF6fubMkwhjPNRryi0BOyas9zlp6JUsNN0RQ4KRma8satN1JwEOAxlhMgJ7OwgRBbwuqCCiwhdylowbq0xpBsXZbhexgZnq4yOUb'] \n",
            "Min: 1 --> ['M' '9' '1']\n",
            "[0.03196347 0.03196347 0.03652968 ... 0.05022831 0.03652968 0.03196347]\n",
            "Rockyou feature:  [1 1 1 ... 1 1 1]\n",
            "JTR feature:  [1 1 1 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate characters weights"
      ],
      "metadata": {
        "id": "0XE_dLdSqh5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary containing occurrencies of all the characters in the dataset\n",
        "def calculate_occurrencies(passwords):\n",
        "  tot_occurrencies = 0\n",
        "  occurrencies_dict = dict()\n",
        "  for password in passwords:\n",
        "    for character in password:\n",
        "      tot_occurrencies += 1\n",
        "      if character in occurrencies_dict:\n",
        "        occurrencies_dict[character] += 1\n",
        "      else:\n",
        "        occurrencies_dict[character] = 1\n",
        "  return occurrencies_dict, tot_occurrencies\n",
        "\n",
        "# Converts occurrencies to weights\n",
        "def calculate_weights(occurrencies_dict, tot_occurrencies):\n",
        "  weights_dict = dict()\n",
        "  for key in occurrencies_dict:\n",
        "    weights_dict[key] = 1 - (occurrencies_dict[key] / tot_occurrencies)\n",
        "  return weights_dict\n",
        "\n",
        "def normalize_weights_dict(weights_dict):\n",
        "  maximum = max(weights_dict.values())\n",
        "  minimum = min(weights_dict.values())\n",
        "  normalized_weights_dict = dict()\n",
        "  for key in weights_dict:\n",
        "    normalized_weights_dict[key] = normalize(weights_dict[key], minimum, maximum)\n",
        "  return normalized_weights_dict\n",
        "\n",
        "\n",
        "occurrencies_dict, tot_occurrencies = calculate_occurrencies(passwords)\n",
        "weights_dict = calculate_weights(occurrencies_dict, tot_occurrencies)\n",
        "normalized_weights_dict = normalize_weights_dict(weights_dict)\n",
        "print(normalized_weights_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "x9qsx0MWqnJa",
        "outputId": "1b7f91de-91ae-4590-99f2-80f57d34e9e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'k': 0.6657636716788613, 'z': 0.7784853687869094, 'd': 0.680859037257116, 'e': 0.2982357306788878, '5': 0.5838229999191711, '7': 0.6315560007935909, 'i': 0.37886109961275827, 'n': 0.4990337300469549, 'o': 0.3892096181212973, '3': 0.47419728562645974, '4': 0.5838548415145818, 'v': 0.8009630857935044, 's': 0.5261848135184723, '1': 0.055509698215155584, 'y': 0.6797029424083516, 'r': 0.5116895395460378, 'm': 0.6072412686671358, 'g': 0.6905658251504523, '2': 0.312214191064269, 'l': 0.5871467726093695, 'a': 0.0, 'b': 0.7437045491842426, 'h': 0.7130508902175275, 'A': 0.8941389420262521, 'V': 0.9695202451312972, 'Y': 0.9480688072383304, 'q': 0.8649475470949451, 'D': 0.9276583445799487, 'E': 0.9401794396369086, 'M': 0.9007007600343888, 'Z': 0.9692263227121206, 'f': 0.8096288984522538, 'N': 0.9141256665303191, 't': 0.6096514325043904, 'u': 0.6063570520561109, '6': 0.6317788919614675, 'c': 0.6825099015114968, '8': 0.6026805724628994, 'w': 0.761756284428726, '9': 0.5184987422569818, '0': 0.3870076493309591, 'j': 0.7666990797778928, 'W': 0.9690891589165042, 'U': 0.9483112932341508, 'I': 0.9435962877598458, 'O': 0.9308841431304217, 'Q': 0.920391112765786, 'P': 0.967134574828975, 'p': 0.7276710812181133, '@': 0.9866583715228365, '-': 0.9922967832640575, 'H': 0.9673648140573302, 'x': 0.8298017738218002, '.': 0.9848703434728412, 'T': 0.9270607023276201, '>': 0.9997722101251383, 'G': 0.9662528075714416, 'J': 0.9671468215964403, '&': 0.998376078634045, '?': 0.9987753232534273, '<': 0.9997134256413018, '!': 0.9946579600314491, 'S': 0.9615353527436432, 'R': 0.9631078376862421, 'F': 0.9682784229102724, 'B': 0.9670757903451386, 'K': 0.9667671718050016, 'X': 0.9695716815546533, 'C': 0.9661450360177425, 'L': 0.9644696782284313, ';': 0.9991133340354816, '_': 0.9928331916790563, '%': 0.9990006637747975, '±': 0.9996766853389043, '\"': 0.9999804051720546, '~': 0.9997746594786316, '+': 0.9983074967362368, '^': 0.9991745678728097, '/': 0.9979596885402106, '$': 0.9972101863713069, ')': 0.9991084353284948, ' ': 0.9972836669761019, '(': 0.9992309030031526, '#': 0.9970436303337731, 'Ú': 0.999938766162672, '*': 0.9954050128468594, '`': 0.9999706077580827, '{': 0.9999069245672613, '}': 0.9999118232742463, '[': 0.9996350463295218, ']': 0.9996791346923978, 'þ': 0.9999436648696588, 'Þ': 0.9999828545255479, 'Ó': 0.9999608103441091, 'Ô': 0.9999902025860282, '=': 0.9994146045151384, '\\\\': 0.9999044752137679, '\\x1c': 0.9999902025860282, '³': 0.9999436648696588, '¿': 0.9999902025860282, '\\x16': 0.9999951012930132, 'Ò': 0.9999975506465066, '·': 0.9999926519395216, '\\x1e': 0.9999975506465066, '\\x19': 1.0, '\\x05': 0.9999951012930132, '\\x1b': 0.9999975506465066, 'Å': 0.9999706077580827, '‚': 1.0, 'Ä': 0.999882431032329, 'à': 0.9999975506465066, 'õ': 0.9999706077580827, 'ß': 0.9999779558185611, '´': 0.9999975506465066, '«': 1.0, 'ð': 0.9999902025860282, 'Ð': 0.9999534622836307, 'å': 0.9999877532325347, 'â': 0.9999975506465066, '°': 0.9999730571115761, '|': 0.9999118232742463, '\\x7f': 0.9999975506465066, '²': 0.9999975506465066, '¾': 0.9999877532325347, 'Ÿ': 0.9999706077580827, '\\x08': 1.0, 'ê': 1.0, 'á': 0.9999902025860282, '\\x10': 0.9999926519395216, '\\x17': 0.9999926519395216, 'º': 0.9999779558185611, '¡': 0.9999951012930132, '÷': 0.9999608103441091, 'Õ': 0.9999975506465066, 'í': 0.9999975506465066, 'ú': 0.9999877532325347, 'µ': 0.9999926519395216, 'Ý': 0.9999926519395216, 'Ü': 0.9999951012930132, 'Û': 0.9999975506465066, 'Ö': 0.9999975506465066, '×': 1.0, '¨': 0.9999926519395216, '\\xa0': 0.9999975506465066, 'æ': 0.9999951012930132, 'è': 1.0, 'ù': 0.9999975506465066, 'É': 1.0, '\\x06': 0.9999975506465066, 'Ñ': 0.9999926519395216, '\\x81': 0.9999975506465066, '\\x11': 0.9999975506465066, 'Â': 0.9999951012930132, 'ý': 1.0, '—': 0.9999975506465066, '›': 0.9999975506465066, '‹': 1.0, 'œ': 1.0, '™': 1.0, 'ó': 1.0, '¦': 1.0, '\\x0f': 1.0, 'ä': 1.0, 'ï': 1.0, 'Ï': 1.0, 'Ç': 0.9999853038790414, 'ò': 1.0, 'ö': 1.0, '\\x12': 0.9999926519395216, '\\x8d': 0.9999975506465066, 'î': 1.0, '¹': 1.0, '¶': 1.0, 'ñ': 1.0, '¼': 0.9999975506465066, 'Ù': 1.0, '…': 1.0, '\\x13': 1.0, '\\x1d': 1.0, '\\x04': 1.0, '\\x0e': 1.0, '\\x02': 1.0, '¯': 1.0, '\\x01': 1.0, 'Ê': 1.0, 'Æ': 0.9999975506465066, '‡': 1.0, '¤': 1.0, 'À': 1.0, '¢': 1.0, 'û': 1.0, 'ƒ': 1.0, 'é': 1.0, 'Á': 1.0, '§': 1.0, 'Í': 0.9999975506465066, 'Ã': 1.0, '»': 1.0, '\\x18': 1.0, '½': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Password structure feature"
      ],
      "metadata": {
        "id": "uxIR4HNJgG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define macros\n",
        "numbers = 0\n",
        "lower_case = 1\n",
        "upper_case = 2\n",
        "special_char = 3\n",
        "\n",
        "# Counts numbers, lowercases, uppercases and other characters in a password\n",
        "def calculate_password_structure(passwords):\n",
        "  passwords_structure = []\n",
        "  for password in passwords:\n",
        "    counts = np.array([0, 0, 0, 0])\n",
        "    for character in password:\n",
        "      if character.isnumeric():\n",
        "        counts[numbers] += 1\n",
        "      elif character.islower():\n",
        "        counts[lower_case] += 1\n",
        "      elif character.isupper():\n",
        "        counts[upper_case] += 1\n",
        "      else:\n",
        "        counts[special_char] += 1\n",
        "\n",
        "    passwords_structure.append(counts / len(password))\n",
        "  return np.array(passwords_structure)\n",
        "\n",
        "feat_structure = calculate_password_structure(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_structure[i])"
      ],
      "metadata": {
        "id": "wLptS-t0gAEK",
        "outputId": "1cb69507-b22c-4f4a-e2f2-127fbdd647c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 [0.5 0.5 0.  0. ]\n",
            "kino3434 [0.5 0.5 0.  0. ]\n",
            "visi7k1yr [0.22222222 0.77777778 0.         0.        ]\n",
            "megzy123 [0.375 0.625 0.    0.   ]\n",
            "lamborghin1 [0.09090909 0.90909091 0.         0.        ]\n",
            "AVYq1lDE4MgAZfNt [0.125  0.3125 0.5625 0.    ]\n",
            "u6c8vhow [0.25 0.75 0.   0.  ]\n",
            "v1118714 [0.875 0.125 0.    0.   ]\n",
            "universe2908 [0.33333333 0.66666667 0.         0.        ]\n",
            "as326159 [0.75 0.25 0.   0.  ]\n",
            "asv5o9yu [0.25 0.75 0.   0.  ]\n",
            "612035180tok [0.75 0.25 0.   0.  ]\n",
            "jytifok873 [0.3 0.7 0.  0. ]\n",
            "WUt9IZzE0OQ7PkNE [0.1875 0.1875 0.625  0.    ]\n",
            "jerusalem393 [0.25 0.75 0.   0.  ]\n",
            "g067057895 [0.9 0.1 0.  0. ]\n",
            "52558000aaa [0.72727273 0.27272727 0.         0.        ]\n",
            "idofo673 [0.375 0.625 0.    0.   ]\n",
            "6975038lp [0.77777778 0.22222222 0.         0.        ]\n",
            "sbl571017 [0.66666667 0.33333333 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Password score feature by characters appearance frequency"
      ],
      "metadata": {
        "id": "Klkyyk59omsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate passwords score with character weights\n",
        "def calculate_password_scores(passwords):\n",
        "  scores = []\n",
        "  for password in passwords:\n",
        "    score = 0\n",
        "    for character in password:\n",
        "      # If weight does not exist in the dictionary, it is a very rare character so is given value 1\n",
        "      try:\n",
        "        score += normalized_weights_dict[character]\n",
        "      except:\n",
        "        score += 1\n",
        "    normalized_score = score / len(password)\n",
        "    scores.append(normalized_score)\n",
        "\n",
        "  return np.array(scores)\n",
        "\n",
        "\n",
        "feat_scores = calculate_password_scores(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_scores[i])"
      ],
      "metadata": {
        "id": "T3vr_4C-o06P",
        "outputId": "0c053435-4b4d-4fca-ed86-9e13a2957cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kzde5577 0.6067627262284123\n",
            "kino3434 0.5061215467177443\n",
            "visi7k1yr 0.5143435501310545\n",
            "megzy123 0.48701903882470265\n",
            "lamborghin1 0.47054663557917564\n",
            "AVYq1lDE4MgAZfNt 0.7849414053410604\n",
            "u6c8vhow 0.6485382870691287\n",
            "v1118714 0.35513666167814983\n",
            "universe2908 0.47833016142056034\n",
            "as326159 0.38777582782024717\n",
            "asv5o9yu 0.5130924067592362\n",
            "612035180tok 0.42952944653592046\n",
            "jytifok873 0.6007950601438756\n",
            "WUt9IZzE0OQ7PkNE 0.8146300190314772\n",
            "jerusalem393 0.47239027508689135\n",
            "g067057895 0.5648297331919243\n",
            "52558000aaa 0.3479442464797782\n",
            "idofo673 0.5481625562432801\n",
            "6975038lp 0.5715955551310014\n",
            "sbl571017 0.4668886869533008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create the feature vector and split dataset\n",
        "\n",
        "def create_feat_dict():\n",
        "  feat_dict = dict()\n",
        "  feat_dict['length'] = feat_length\n",
        "  feat_dict['rockyou'] = feat_rockyou\n",
        "  feat_dict['john_the_ripper'] = feat_jtr\n",
        "\n",
        "  # divide \"feat_structure\" in its sub-features\n",
        "  for i, string in enumerate([\"numbers\", \"lowercase\", \"uppercase\", \"special\"]):\n",
        "    feat_dict[string] = feat_structure[:, i]\n",
        "\n",
        "  feat_dict['scores'] = feat_scores\n",
        "\n",
        "  return feat_dict\n",
        "\n",
        "\n",
        "def create_feature_vector(features_list, n_feat):\n",
        "  feature_vector = np.zeros(shape=(n_samples, n_feat))\n",
        "\n",
        "  for i, feature in enumerate(features_list):\n",
        "    # \"-1\" allows to let Python compute the remaining dimension\n",
        "    feature = feature.reshape(n_samples, -1)\n",
        "    feature_vector[:, i:i+1] = feature\n",
        "\n",
        "  return feature_vector\n",
        "\n",
        "\n",
        "\n",
        "feat_dict = create_feat_dict()\n",
        "\n",
        "for feature in feat_dict:\n",
        "  print(f\"Feature {feature}: {feat_dict[feature]}\")\n",
        "\n",
        "\n",
        "feature_vector = create_feature_vector(feat_dict.values(), n_features)\n",
        "print(\"\\n\")\n",
        "print(feature_vector.shape)\n",
        "print(feature_vector)\n",
        "\n",
        "# 80% of the dataset is used for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.20, random_state=42)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OzjdbbivBUF",
        "outputId": "1582e232-a3ce-44d9-85e7-9919c9d2ee77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature length: [0.03196347 0.03196347 0.03652968 ... 0.05022831 0.03652968 0.03196347]\n",
            "Feature rockyou: [1 1 1 ... 1 1 1]\n",
            "Feature john_the_ripper: [1 1 1 ... 1 1 1]\n",
            "Feature numbers: [0.5        0.5        0.22222222 ... 0.5        0.22222222 0.25      ]\n",
            "Feature lowercase: [0.5        0.5        0.77777778 ... 0.5        0.77777778 0.75      ]\n",
            "Feature uppercase: [0. 0. 0. ... 0. 0. 0.]\n",
            "Feature special: [0. 0. 0. ... 0. 0. 0.]\n",
            "Feature scores: [0.60676273 0.50612155 0.51434355 ... 0.43682709 0.35626581 0.72711691]\n",
            "\n",
            "\n",
            "(669639, 8)\n",
            "[[0.03196347 1.         1.         ... 0.         0.         0.60676273]\n",
            " [0.03196347 1.         1.         ... 0.         0.         0.50612155]\n",
            " [0.03652968 1.         1.         ... 0.         0.         0.51434355]\n",
            " ...\n",
            " [0.05022831 1.         1.         ... 0.         0.         0.43682709]\n",
            " [0.03652968 1.         1.         ... 0.         0.         0.35626581]\n",
            " [0.03196347 1.         1.         ... 0.         0.         0.72711691]]\n",
            "(535711, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-parameters search through 5-fold cross validation and training of LinearSVC and LogisticRegression classifiers"
      ],
      "metadata": {
        "id": "xUJYyE2T-Sgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LinearSVC*"
      ],
      "metadata": {
        "id": "i37nUdoL-m9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "svc_max_iter = 10000\n",
        "lr_max_iter = 1000\n",
        "\n",
        "clf = LinearSVC(dual=False, max_iter=svc_max_iter) # default max_iter = 1000\n",
        "\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")\n"
      ],
      "metadata": {
        "id": "-qpWcgh-22Yl",
        "outputId": "782d49c8-7bba-47b0-8d0b-fc50077acb00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.178006092826 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters search\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "param_distributions = {'C': [0.1, 1, 10, 100, 1000],\n",
        "                       'penalty': ['l1', 'l2']}\n",
        "\n",
        "svc_cv = HalvingRandomSearchCV(LinearSVC(dual=False, max_iter=svc_max_iter),\n",
        "                               param_distributions, n_jobs=-1)\n",
        "svc_cv.fit(X_train, y_train)\n",
        "print(f\"\\nAccuracy (CV): {svc_cv.best_score_ * 100} %\")\n",
        "print(\"Best parameters: \", svc_cv.best_params_)\n",
        "\n",
        "\n",
        "# Train a new SVC with best hyperparameters\n",
        "\n",
        "svc = LinearSVC(C=svc_cv.best_params_['C'], penalty=svc_cv.best_params_['penalty'],\n",
        "                dual=False, max_iter=svc_max_iter)\n",
        "\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "accuracy = svc.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with best parameters: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KeD5AkYT23nx",
        "outputId": "1a1a72fb-9eea-4e55-8ed3-9fd29cdb4a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 10 is smaller than n_iter=17857. Running 10 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy (CV): 96.98113207547169 %\n",
            "Best parameters:  {'penalty': 'l1', 'C': 1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LogisticRegression*"
      ],
      "metadata": {
        "id": "Lll_Zbnb_JG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=lr_max_iter)  # default max_iter = 100\n",
        "\n",
        "# Training without cross validation\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "upr_-l7k_NvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using HalvingRandomized on LogisticRegression\n",
        "\n",
        "param_distributions = {'C': [0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "lr_cv = HalvingRandomSearchCV(LogisticRegression(max_iter=lr_max_iter),\n",
        "                               param_distributions,\n",
        "                               scoring='accuracy', n_jobs=-1, verbose=3)\n",
        "lr_cv.fit(X_train, y_train)\n",
        "print(f\"\\nAccuracy (CV): {lr_cv.best_score_ * 100} %\")\n",
        "print(\"Best parameters: \", lr_cv.best_params_)\n",
        "\n",
        "\n",
        "# Train a new LogisticRegression with best hyperparameters\n",
        "lr = LogisticRegression(C=lr_cv.best_params_['C'], max_iter=lr_max_iter)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "predictions = lr.predict(X_test)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = lr.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with best parameters: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dv17HBYTmR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "T--IzlYmc4dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "# Training without cross validation\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")"
      ],
      "metadata": {
        "id": "aLxulYAsc9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: Dataset Creation"
      ],
      "metadata": {
        "id": "I7XC8I8m-CAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "trainset = CustomDataset(X_train, y_train)\n",
        "testset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "rEQlGb3Y-FES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: Trainloader creation"
      ],
      "metadata": {
        "id": "WqLB76e--R4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_DataLoader(trainset, testset):\n",
        "  # trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "  # No need to shuffle test data.\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return trainloader, testloader\n",
        "\n",
        "trainloader, testloader = create_DataLoader(trainset, testset)"
      ],
      "metadata": {
        "id": "1_bBkaz_-V0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: initialization"
      ],
      "metadata": {
        "id": "Y7Dzs4JiyXOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.mid_n = int((n_input + n_output) / 2)\n",
        "\n",
        "    # Define Layers:\n",
        "    self.l1 = nn.Linear(self.n_input, self.mid_n) # layer 1\n",
        "    self.l2 = nn.Linear(self.mid_n, self.n_output) # layer 2\n",
        "    self.l3 = nn.Linear(self.n_output, self.n_output) # layer 3\n",
        "    self.double()\n",
        "\n",
        "    # Define Activation functions:\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # Weights initialization\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='relu') # Using HE because more optimized for ReLU activated layers\n",
        "    nn.init.zeros_(self.l1.bias)\n",
        "    nn.init.kaiming_normal_(self.l2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.l2.bias)\n",
        "    nn.init.normal_(self.l3.weight) # Using normal distribution for LogSoftMax activated layer\n",
        "    nn.init.normal_(self.l3.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Layers: 3\n",
        "    Activation Functions:\n",
        "    RELU for first two layers\n",
        "    Log Softmax for last layer\n",
        "    '''\n",
        "    x = self.l1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def train_model(self, tr_loader, n_epochs, criterion, optimizer):\n",
        "    # losses --> {[*idx_first_epoch*]: loss_1, [*idx_second_epoch*]: loss_2, ...}\n",
        "    losses = {}\n",
        "    for e in range(n_epochs):\n",
        "      for features, labels in tr_loader:\n",
        "        optimizer.zero_grad() # set optimizer gradients to zero:\n",
        "        output = self(features) # Initial output (method \"forward()\" automatically called)\n",
        "        loss = criterion(output, labels.long()) # Loss Calculation\n",
        "        loss.backward() # Pass loss function gradients to previous layers:\n",
        "        optimizer.step() # Update Weights\n",
        "        losses[e] = loss.item()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "  def test_model(self, ts_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in ts_loader:\n",
        "      outputs = self(features)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "# Define macros.\n",
        "number_of_features = 8 # Rockyou, johntheripper , password len, numbers/len, lowercases/len, uppercases/len, special characters/len, password score\n",
        "number_of_outputs = 3 # Unsecure, intermediate, secure\n",
        "\n",
        "# Initialize NN\n",
        "NN = NeuralNetwork(number_of_features, number_of_outputs)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(NN.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4FDFyMMQypbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network training: training and testing"
      ],
      "metadata": {
        "id": "ObgSGLWrzMNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "n_epochs = 5\n",
        "losses = NN.train_model(trainloader, n_epochs, criterion, optimizer)\n",
        "print(losses)\n",
        "\n",
        "# Test the models\n",
        "accuracy = NN.test_model(testloader)\n",
        "print(f\"Accuracy of the model on the test samples: {accuracy * 100} %\")"
      ],
      "metadata": {
        "id": "SRgndDBlzQgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating weight of each feature on classifier decision"
      ],
      "metadata": {
        "id": "hLOkllw1_WKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weigh_features(model, X_train, X_test, y_train, y_test):\n",
        "  # return a list with several accuracy values\n",
        "  n_models = X_train.shape[1]\n",
        "  features_weights = list()\n",
        "\n",
        "  for i in range(n_models):\n",
        "    sub_x_train = X_train[:, i].reshape(-1, 1)\n",
        "    sub_x_test = X_test[:, i].reshape(-1, 1)\n",
        "\n",
        "    model.fit(sub_x_train, y_train)\n",
        "    accuracy = model.score(sub_x_test, y_test)\n",
        "\n",
        "    features_weights.append(accuracy)\n",
        "\n",
        "  return features_weights\n",
        "\n",
        "\n",
        "def weigh_features_nn(X_train, X_test, y_train, y_test):\n",
        "  n_input = 1\n",
        "  n_output = 3\n",
        "\n",
        "  # return a list with several accuracy values\n",
        "  n_models = X_train.shape[1]\n",
        "  features_weights = list()\n",
        "\n",
        "  for i in range(n_models):\n",
        "    sub_x_train = X_train[:, i].reshape(-1, 1)\n",
        "    sub_x_test = X_test[:, i].reshape(-1, 1)\n",
        "\n",
        "    trainset = CustomDataset(sub_x_train, y_train)\n",
        "    testset = CustomDataset(sub_x_test, y_test)\n",
        "    trainloader, testloader = create_DataLoader(trainset, testset)\n",
        "    NN = NeuralNetwork(n_input, n_output)\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(NN.parameters(), lr=0.001)\n",
        "    losses = NN.train_model(trainloader, n_epochs, criterion, optimizer)\n",
        "    accuracy = NN.test_model(testloader)\n",
        "\n",
        "    features_weights.append(accuracy)\n",
        "\n",
        "  return features_weights"
      ],
      "metadata": {
        "id": "brsnOAv7_Z1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "svc = LinearSVC(max_iter=1000, dual=False)\n",
        "svc_feat_w = weigh_features(svc, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nLinearSVC - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {svc_feat_w[i]}\")\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr_feat_w = weigh_features(lr, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nLogisticRegression - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {lr_feat_w[i]}\")\n",
        "\n",
        "knn=KNeighborsClassifier()\n",
        "knn_feat_w=weigh_features(knn, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nKNN - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {knn_feat_w[i]}\")\n",
        "\n",
        "nn_feat_w = weigh_features_nn(X_train, X_test, y_train, y_test)\n",
        "print(\"\\nANN - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {nn_feat_w[i]}\")"
      ],
      "metadata": {
        "id": "Zxiwh5qO_b2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}