{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize as nrm\n",
        "import random\n",
        "from importlib.util import find_spec\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn # Contains Required functions and layers\n",
        "import torch.nn.functional as F # For neural network functions:\n",
        "import torch.optim as optim # Contains Optimization function available in PyTorch.\n",
        "project_folder = \"/content/password_strength_classifier\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KykGU56SuABu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of useful directories and download of password dictionaries"
      ],
      "metadata": {
        "id": "V8T3qe2zMghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def download_file(url, dest_folder_name):\n",
        "  local_filename = url.split('/')[-1]\n",
        "  path = os.path.join(\"/{}/{}\".format(dest_folder_name, local_filename))\n",
        "\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(requests.get(url, stream=True).content)\n",
        "  return path\n",
        "\n",
        "\n",
        "def read_file(filepath):\n",
        "  with open(filepath, errors='replace', encoding='utf-8') as f:\n",
        "    data = {line.split('\\n')[0] for line in f.readlines()}\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(project_folder) is False:\n",
        "  dict_dir = project_folder + \"/dictionaries/\"\n",
        "  dataset_dir = project_folder + \"/dataset/\"\n",
        "\n",
        "  os.mkdir(project_folder)\n",
        "  os.mkdir(dataset_dir)\n",
        "  os.mkdir(dict_dir)\n",
        "\n",
        "  f_rockyou = download_file(\n",
        "    \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\",\n",
        "    dict_dir)\n",
        "\n",
        "  f_jtr = download_file(\n",
        "      \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Software/john-the-ripper.txt\",\n",
        "      dict_dir)\n",
        "\n",
        "\n",
        "data_rockyou = read_file(f_rockyou)\n",
        "data_jtr = read_file(f_jtr)\n"
      ],
      "metadata": {
        "id": "ynOVFhD_5q3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "\n",
        "if find_spec(\"kaggle\") is None:\n",
        "  ! pip install -q kaggle\n",
        "\n",
        "if os.path.isdir(\"/root/.kaggle\") is False:\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! touch \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "  token = {\"username\":\"matteoturnu\",\"key\":\"79ea644685a3e574038b40e4019b0927\"}\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d bhavikbb/password-strength-classifier-dataset -p $dataset_dir"
      ],
      "metadata": {
        "id": "y0pIiUJswG7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read password dataset\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"password-strength-classifier-dataset.zip\")\n",
        "\n",
        "pswd_df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "print(pswd_df)"
      ],
      "metadata": {
        "id": "8jiwHeBuGRgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VFws5SNT4xt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# remove missing values\n",
        "pswd_df.dropna(inplace=True)\n",
        "print(pswd_df)\n",
        "psw_array = np.array(pswd_df)\n",
        "print(psw_array)\n",
        "\n",
        "# divide into X (passwords) and y (labels) arrays\n",
        "labels = np.array([p[1] for p in psw_array])\n",
        "passwords = np.array([p[0] for p in psw_array])\n",
        "\n",
        "\n",
        "print(\"Len of UNIQUE passwords: \", len(np.unique(passwords)))\n",
        "\n",
        "n_samples = len(passwords)\n",
        "print(\"Number of samples: \", n_samples)\n",
        "n_features = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(value, min, max):\n",
        "  return (value - min) / (max - min)\n",
        "\n",
        "# LENGTH feature\n",
        "lengths = np.array([len(p) for p in passwords])\n",
        "max_len = np.max(lengths)\n",
        "min_len = np.min(lengths)\n",
        "print(\"Lengths shape: \", lengths.shape)\n",
        "print(\"Lenghts:\", lengths)\n",
        "print(f\"Max: {max_len} --> {passwords[lengths == max_len]} \\nMin: {min_len} --> {passwords[lengths == min_len]}\")\n",
        "\n",
        "feat_length = normalize(lengths, min_len, max_len)\n",
        "print(feat_length)\n",
        "\n",
        "\n",
        "\n",
        "# ROCKYOU feature\n",
        "# numpy array of 1 and 0 (0 if rockyou file contains the current password, otherwise is 1)\n",
        "# int() used to convert boolean into number\n",
        "feat_rockyou = np.array([int(p not in data_rockyou) for p in passwords])\n",
        "print(\"Rockyou feature: \", feat_rockyou)\n",
        "\n",
        "\n",
        "# JTR feature\n",
        "feat_jtr = np.array([int(p not in data_jtr) for p in passwords])\n",
        "print(\"JTR feature: \", feat_jtr)\n"
      ],
      "metadata": {
        "id": "Mb6BzStWQbxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate characters weights"
      ],
      "metadata": {
        "id": "0XE_dLdSqh5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary containing occurrencies of all the characters in the dataset\n",
        "def calculate_occurrencies(passwords):\n",
        "  tot_occurrencies = 0\n",
        "  occurrencies_dict = dict()\n",
        "  for password in passwords:\n",
        "    for character in password:\n",
        "      tot_occurrencies += 1\n",
        "      if character in occurrencies_dict:\n",
        "        occurrencies_dict[character] += 1\n",
        "      else:\n",
        "        occurrencies_dict[character] = 1\n",
        "  return occurrencies_dict, tot_occurrencies\n",
        "\n",
        "# Converts occurrencies to weights\n",
        "def calculate_weights(occurrencies_dict, tot_occurrencies):\n",
        "  weights_dict = dict()\n",
        "  for key in occurrencies_dict:\n",
        "    weights_dict[key] = 1 - (occurrencies_dict[key] / tot_occurrencies)\n",
        "  return weights_dict\n",
        "\n",
        "def normalize_weights_dict(weights_dict):\n",
        "  maximum = max(weights_dict.values())\n",
        "  minimum = min(weights_dict.values())\n",
        "  normalized_weights_dict = dict()\n",
        "  for key in weights_dict:\n",
        "    normalized_weights_dict[key] = normalize(weights_dict[key], minimum, maximum)\n",
        "  return normalized_weights_dict\n",
        "\n",
        "\n",
        "occurrencies_dict, tot_occurrencies = calculate_occurrencies(passwords)\n",
        "weights_dict = calculate_weights(occurrencies_dict, tot_occurrencies)\n",
        "normalized_weights_dict = normalize_weights_dict(weights_dict)\n",
        "print(normalized_weights_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "x9qsx0MWqnJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Password structure feature"
      ],
      "metadata": {
        "id": "uxIR4HNJgG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define macros\n",
        "numbers = 0\n",
        "lower_case = 1\n",
        "upper_case = 2\n",
        "special_char = 3\n",
        "\n",
        "# Counts numbers, lowercases, uppercases and other characters in a password\n",
        "def calculate_password_structure(passwords):\n",
        "  passwords_structure = []\n",
        "  for password in passwords:\n",
        "    counts = np.array([0, 0, 0, 0])\n",
        "    for character in password:\n",
        "      if character.isnumeric():\n",
        "        counts[numbers] += 1\n",
        "      elif character.islower():\n",
        "        counts[lower_case] += 1\n",
        "      elif character.isupper():\n",
        "        counts[upper_case] += 1\n",
        "      else:\n",
        "        counts[special_char] += 1\n",
        "\n",
        "    passwords_structure.append(counts / len(password))\n",
        "  return np.array(passwords_structure)\n",
        "\n",
        "feat_structure = calculate_password_structure(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_structure[i])"
      ],
      "metadata": {
        "id": "wLptS-t0gAEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Password score feature by characters appearance frequency"
      ],
      "metadata": {
        "id": "Klkyyk59omsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate passwords score with character weights\n",
        "def calculate_password_scores(passwords):\n",
        "  scores = []\n",
        "  for password in passwords:\n",
        "    score = 0\n",
        "    for character in password:\n",
        "      # If weight does not exist in the dictionary, it is a very rare character so 1.001 points are given\n",
        "      try:\n",
        "        score += normalized_weights_dict[character]\n",
        "      except:\n",
        "        score += 1\n",
        "    normalized_score = score / len(password)\n",
        "    scores.append(normalized_score)\n",
        "\n",
        "  return np.array(scores)\n",
        "\n",
        "\n",
        "feat_scores = calculate_password_scores(passwords)\n",
        "for i in range(20):\n",
        "  print(passwords[i], feat_scores[i])"
      ],
      "metadata": {
        "id": "T3vr_4C-o06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create the feature vector and split dataset\n",
        "\n",
        "def create_feat_dict():\n",
        "  feat_dict = dict()\n",
        "  feat_dict['length'] = feat_length\n",
        "  feat_dict['rockyou'] = feat_rockyou\n",
        "  feat_dict['john_the_ripper'] = feat_jtr\n",
        "\n",
        "  # divide \"feat_structure\" in its sub-features\n",
        "  for i, string in enumerate([\"numbers\", \"lowercase\", \"uppercase\", \"special\"]):\n",
        "    feat_dict[string] = feat_structure[:, i]\n",
        "\n",
        "  feat_dict['scores'] = feat_scores\n",
        "\n",
        "  return feat_dict\n",
        "\n",
        "\n",
        "def create_feature_vector(features_list, n_feat):\n",
        "  feature_vector = np.zeros(shape=(n_samples, n_feat))\n",
        "\n",
        "  for i, feature in enumerate(features_list):\n",
        "    # \"-1\" allows to let Python compute the remaining dimension\n",
        "    feature = feature.reshape(n_samples, -1)\n",
        "    feature_vector[:, i:i+1] = feature\n",
        "\n",
        "  return feature_vector\n",
        "\n",
        "\n",
        "\n",
        "feat_dict = create_feat_dict()\n",
        "\n",
        "for feature in feat_dict:\n",
        "  print(f\"Feature {feature}: {feat_dict[feature]}\")\n",
        "\n",
        "\n",
        "feature_vector = create_feature_vector(feat_dict.values(), n_features)\n",
        "print(\"\\n\")\n",
        "print(feature_vector.shape)\n",
        "print(feature_vector)\n",
        "\n",
        "# 80% of the dataset is used for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.20, random_state=42)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OzjdbbivBUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-parameters search through 5-fold cross validation and training of LinearSVC and LogisticRegression classifiers"
      ],
      "metadata": {
        "id": "xUJYyE2T-Sgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LinearSVC*"
      ],
      "metadata": {
        "id": "i37nUdoL-m9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "svc_max_iter = 10000\n",
        "lr_max_iter = 1000\n",
        "\n",
        "clf = LinearSVC(dual=False, max_iter=svc_max_iter) # default max_iter = 1000\n",
        "\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")\n"
      ],
      "metadata": {
        "id": "-qpWcgh-22Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters search\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "param_distributions = {'C': [0.1, 1, 10, 100, 1000],\n",
        "                       'penalty': ['l1', 'l2']}\n",
        "\n",
        "svc_cv = HalvingRandomSearchCV(LinearSVC(dual=False, max_iter=svc_max_iter),\n",
        "                               param_distributions, n_jobs=-1)\n",
        "svc_cv.fit(X_train, y_train)\n",
        "print(f\"\\nAccuracy (CV): {svc_cv.best_score_ * 100} %\")\n",
        "print(\"Best parameters: \", svc_cv.best_params_)\n",
        "\n",
        "\n",
        "# Train a new SVC with best hyperparameters\n",
        "\n",
        "svc = LinearSVC(C=svc_cv.best_params_['C'], penalty=svc_cv.best_params_['penalty'],\n",
        "                dual=False, max_iter=svc_max_iter)\n",
        "\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "accuracy = svc.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with best parameters: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KeD5AkYT23nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LogisticRegression*"
      ],
      "metadata": {
        "id": "Lll_Zbnb_JG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=lr_max_iter)  # default max_iter = 100\n",
        "\n",
        "# Training without cross validation\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "upr_-l7k_NvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using HalvingRandomized on LogisticRegression\n",
        "\n",
        "param_distributions = {'C': [0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "lr_cv = HalvingRandomSearchCV(LogisticRegression(max_iter=lr_max_iter),\n",
        "                               param_distributions,\n",
        "                               scoring='accuracy', n_jobs=-1, verbose=3)\n",
        "lr_cv.fit(X_train, y_train)\n",
        "print(f\"\\nAccuracy (CV): {lr_cv.best_score_ * 100} %\")\n",
        "print(\"Best parameters: \", lr_cv.best_params_)\n",
        "\n",
        "\n",
        "# Train a new LogisticRegression with best hyperparameters\n",
        "lr = LogisticRegression(C=lr_cv.best_params_['C'], max_iter=lr_max_iter)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "predictions = lr.predict(X_test)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = lr.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy with best parameters: {accuracy * 100} %\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dv17HBYTmR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "T--IzlYmc4dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "# Training without cross validation\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100} %\")"
      ],
      "metadata": {
        "id": "aLxulYAsc9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: Dataset Creation"
      ],
      "metadata": {
        "id": "I7XC8I8m-CAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "trainset = CustomDataset(X_train, y_train)\n",
        "testset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "rEQlGb3Y-FES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: Trainloader creation"
      ],
      "metadata": {
        "id": "WqLB76e--R4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_DataLoader(trainset, testset):\n",
        "  # trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "  # No need to shuffle test data.\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return trainloader, testloader\n",
        "\n",
        "trainloader, testloader = create_DataLoader(trainset, testset)"
      ],
      "metadata": {
        "id": "1_bBkaz_-V0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network: initialization"
      ],
      "metadata": {
        "id": "Y7Dzs4JiyXOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, n_input, n_output):\n",
        "    super().__init__()\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.mid_n = int((n_input + n_output) / 2)\n",
        "\n",
        "    # Define Layers:\n",
        "    self.l1 = nn.Linear(self.n_input, self.mid_n) # layer 1\n",
        "    self.l2 = nn.Linear(self.mid_n, self.n_output) # layer 2\n",
        "    self.l3 = nn.Linear(self.n_output, self.n_output) # layer 3\n",
        "    self.double()\n",
        "\n",
        "    # Define Activation functions:\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # Weights initialization\n",
        "    nn.init.kaiming_normal_(self.l1.weight, mode='fan_in', nonlinearity='relu') # Using HE because more optimized for ReLU activated layers\n",
        "    nn.init.zeros_(self.l1.bias)\n",
        "    nn.init.kaiming_normal_(self.l2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.l2.bias)\n",
        "    nn.init.normal_(self.l3.weight) # Using normal distribution for LogSoftMax activated layer\n",
        "    nn.init.normal_(self.l3.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Layers: 3\n",
        "    Activation Functions:\n",
        "    RELU for first two layers\n",
        "    Log Softmax for last layer\n",
        "    '''\n",
        "    x = self.l1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def train_model(self, tr_loader, n_epochs, criterion, optimizer):\n",
        "    # losses --> {[*idx_first_epoch*]: loss_1, [*idx_second_epoch*]: loss_2, ...}\n",
        "    losses = {}\n",
        "    for e in range(n_epochs):\n",
        "      for features, labels in tr_loader:\n",
        "        optimizer.zero_grad() # set optimizer gradients to zero:\n",
        "        output = self(features) # Initial output (method \"forward()\" automatically called)\n",
        "        loss = criterion(output, labels.long()) # Loss Calculation\n",
        "        loss.backward() # Pass loss function gradients to previous layers:\n",
        "        optimizer.step() # Update Weights\n",
        "        losses[e] = loss.item()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "  def test_model(self, ts_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, labels in ts_loader:\n",
        "      outputs = self(features)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "# Define macros.\n",
        "number_of_features = 8 # Rockyou, johntheripper , password len, numbers/len, lowercases/len, uppercases/len, special characters/len, password score\n",
        "number_of_outputs = 3 # Unsecure, intermediate, secure\n",
        "\n",
        "# Initialize NN\n",
        "NN = NeuralNetwork(number_of_features, number_of_outputs)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(NN.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4FDFyMMQypbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network training: training and testing"
      ],
      "metadata": {
        "id": "ObgSGLWrzMNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "n_epochs = 5\n",
        "losses = NN.train_model(trainloader, n_epochs, criterion, optimizer)\n",
        "print(losses)\n",
        "\n",
        "# Test the models\n",
        "accuracy = NN.test_model(testloader)\n",
        "print(f\"Accuracy of the model on the test samples: {accuracy * 100} %\")"
      ],
      "metadata": {
        "id": "SRgndDBlzQgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating weight of each feature on classifier decision"
      ],
      "metadata": {
        "id": "hLOkllw1_WKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weigh_features(model, X_train, X_test, y_train, y_test):\n",
        "  # return a list with several accuracy values\n",
        "  n_models = X_train.shape[1]\n",
        "  features_weights = list()\n",
        "\n",
        "  for i in range(n_models):\n",
        "    sub_x_train = X_train[:, i].reshape(-1, 1)\n",
        "    sub_x_test = X_test[:, i].reshape(-1, 1)\n",
        "\n",
        "    model.fit(sub_x_train, y_train)\n",
        "    accuracy = model.score(sub_x_test, y_test)\n",
        "\n",
        "    features_weights.append(accuracy)\n",
        "\n",
        "  return features_weights\n",
        "\n",
        "\n",
        "def weigh_features_nn(X_train, X_test, y_train, y_test):\n",
        "  n_input = 1\n",
        "  n_output = 3\n",
        "\n",
        "  # return a list with several accuracy values\n",
        "  n_models = X_train.shape[1]\n",
        "  features_weights = list()\n",
        "\n",
        "  for i in range(n_models):\n",
        "    sub_x_train = X_train[:, i].reshape(-1, 1)\n",
        "    sub_x_test = X_test[:, i].reshape(-1, 1)\n",
        "\n",
        "    trainset = CustomDataset(sub_x_train, y_train)\n",
        "    testset = CustomDataset(sub_x_test, y_test)\n",
        "    trainloader, testloader = create_DataLoader(trainset, testset)\n",
        "    NN = NeuralNetwork(n_input, n_output)\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(NN.parameters(), lr=0.001)\n",
        "    losses = NN.train_model(trainloader, n_epochs, criterion, optimizer)\n",
        "    accuracy = NN.test_model(testloader)\n",
        "\n",
        "    features_weights.append(accuracy)\n",
        "\n",
        "  return features_weights"
      ],
      "metadata": {
        "id": "brsnOAv7_Z1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: in this case best parameters are not taken into account\n",
        "\n",
        "svc = LinearSVC(max_iter=1000, dual=False)\n",
        "svc_feat_w = weigh_features(svc, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nLinearSVC - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {svc_feat_w[i]}\")\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr_feat_w = weigh_features(lr, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nLogisticRegression - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {lr_feat_w[i]}\")\n",
        "\n",
        "knn=KNeighborsClassifier()\n",
        "knn_feat_w=weigh_features(knn, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nKNN - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {knn_feat_w[i]}\")\n",
        "\n",
        "nn_feat_w = weigh_features_nn(X_train, X_test, y_train, y_test)\n",
        "print(\"\\nANN - Accuracies on single features: \\n\")\n",
        "for i, feat in enumerate(feat_dict):\n",
        "  print(f\"Feature '{feat}' weight: {nn_feat_w[i]}\")"
      ],
      "metadata": {
        "id": "Zxiwh5qO_b2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}